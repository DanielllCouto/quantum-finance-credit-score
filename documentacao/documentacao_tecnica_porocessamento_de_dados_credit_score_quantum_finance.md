# Documenta√ß√£o T√©cnica: Processamento de Dados para Previs√£o de Score de Cr√©dito - Quantum Finance

**Autor:** Daniel Estrella Couto

**Projeto:** Quantum Finance - Credit Score Prediction

**Notebook:** processamento-dados.ipynb



---



## 1. Introdu√ß√£o

Este documento detalha as estrat√©gias de pr√©-processamento de dados, tratamento de valores ausentes e inconsist√™ncias, feature engineering e an√°lises explorat√≥rias realizadas em um projeto de Machine Learning focado na previs√£o de score de cr√©dito. O objetivo √© fornecer uma vis√£o abrangente e profissional das etapas executadas, justificando cada decis√£o t√©cnica e demonstrando a robustez da abordagem.

## 2. An√°lise Preliminar e Carregamento dos Dados

Nesta se√ß√£o, descrevemos o processo inicial de carregamento do dataset e a primeira inspe√ß√£o dos dados, que √© crucial para entender a estrutura, os tipos de vari√°veis e a presen√ßa de valores ausentes ou inconsistentes.

### 2.1. Carregamento do Dataset

O dataset foi carregado utilizando a biblioteca `pandas` do Python. Esta etapa √© fundamental para iniciar qualquer projeto de an√°lise de dados, garantindo que os dados estejam acess√≠veis e em um formato adequado para manipula√ß√£o.

```python
import pandas as pd

# Carregamento do dataset
df = pd.read_csv('nome_do_arquivo.csv') # Substituir pelo nome real do arquivo
```

### 2.2. Vis√£o Geral Inicial dos Dados

Ap√≥s o carregamento, uma an√°lise inicial foi realizada para obter uma compreens√£o b√°sica da estrutura do DataFrame. Isso incluiu a verifica√ß√£o das primeiras linhas, o formato das colunas e a identifica√ß√£o de informa√ß√µes gerais sobre o dataset.

```python
df.head()
df.info()
df.describe()
```

### 2.3. Identifica√ß√£o de Tipos de Dados e Valores Ausentes

Uma etapa cr√≠tica no pr√©-processamento √© a identifica√ß√£o dos tipos de dados de cada coluna e a detec√ß√£o de valores ausentes. Valores ausentes podem impactar significativamente a performance de modelos de Machine Learning e, portanto, precisam ser tratados adequadamente. A an√°lise foi feita para quantificar a presen√ßa de nulos e entender a natureza de cada coluna.

```python
df.isnull().sum()
df.dtypes
```

## 3. Tratamento de Valores Ausentes e Inconsist√™ncias

Esta se√ß√£o aborda as estrat√©gias detalhadas para lidar com valores ausentes e inconsist√™ncias em colunas espec√≠ficas, garantindo a qualidade e a integridade dos dados para as fases subsequentes.

### 3.1. Coluna `mix_credito`: An√°lise de Data Leakage e Remo√ß√£o

A coluna `mix_credito` foi identificada como tendo uma forte correla√ß√£o com a vari√°vel target (`score_credito`). Uma an√°lise aprofundada revelou que essa correla√ß√£o era t√£o alta que poderia levar a um problema de *data leakage*. O *data leakage* ocorre quando informa√ß√µes do target s√£o inadvertidamente inclu√≠das nas features de treinamento, fazendo com que o modelo

aparentemente performe bem no treinamento, mas falhe miseravelmente em dados novos e n√£o vistos. Para evitar que o modelo "trapaceasse" e para garantir que ele aprendesse padr√µes reais do comportamento do cliente, a decis√£o foi remover a coluna `mix_credito` do dataset antes do treinamento. Esta √© uma pr√°tica crucial para a constru√ß√£o de modelos robustos e generaliz√°veis.

```python
# Calcular distribui√ß√£o percentual de mix_credito (ignorando os nulos "_")
mix_credito_sem_nulo = df_categoricas[df_categoricas["mix_credito"] != "_"]["mix_credito"].value_counts(normalize=True) * 100

# Calcular distribui√ß√£o percentual de score_credito
score_credito_pct = df_categoricas["score_credito"].value_counts(normalize=True) * 100

# Exibir os resultados
print("=== Distribui√ß√£o percentual de mix_credito (sem nulos) ===")
print(mix_credito_sem_nulo.round(2))

print("\n=== Distribui√ß√£o percentual de score_credito ===")
print(score_credito_pct.round(2))

# Remover a coluna mix_credito do dataframe categ√≥rico
df_categoricas = df_categoricas.drop(columns=["mix_credito"])

# Conferir se a coluna foi removida
print("Colunas restantes em df_categoricas:")
print(df_categoricas.columns.tolist())
```

### 3.2. Coluna `pagamento_valor_minimo`: Tratamento de Valores 'NM'

A coluna `pagamento_valor_minimo` apresentava valores `NM` (Not Mentioned/N√£o Mencionado), que representavam dados ausentes. Inicialmente, foi levantada a hip√≥tese de que o `comportamento_pagamento` do cliente poderia ser utilizado para inferir esses valores. A l√≥gica sugeria que clientes com `Small_value_payments` (pagamentos de baixo valor) poderiam ter maior probabilidade de pagar apenas o valor m√≠nimo (`Yes`), enquanto outros com `Medium` ou `Large_value_payments` (pagamentos de valor m√©dio ou alto) tenderiam a n√£o pagar apenas o m√≠nimo (`No`).

Para validar essa hip√≥tese, foi constru√≠da uma tabela de conting√™ncia cruzando `pagamento_valor_minimo` e `comportamento_pagamento`. A an√°lise revelou que a distribui√ß√£o de `Yes` e `No` era relativamente equilibrada em todos os comportamentos, com `Yes` sendo majorit√°rio, mas sem uma domin√¢ncia absoluta (variando entre 44% e 57%). O percentual de `NM` tamb√©m se manteve est√°vel em todos os grupos, em torno de 11-12%, sem concentra√ß√£o em um comportamento espec√≠fico. Essa evid√™ncia estat√≠stica indicou que n√£o havia uma correla√ß√£o forte o suficiente para justificar a imputa√ß√£o baseada no `comportamento_pagamento`.

Diante disso, a abordagem mais adequada foi manter `NM` como uma categoria pr√≥pria, renomeando-a para `Not Informed` (N√£o Informado). Essa decis√£o preserva a informa√ß√£o de que o cliente n√£o respondeu a essa quest√£o, tratando os valores ausentes como uma categoria significativa em si, em vez de imput√°-los de forma arbitr√°ria e potencialmente enviesada.

```python
# Exibir os 30 primeiros valores √∫nicos
df_categoricas.pagamento_valor_minimo.unique()[:30]

# Ver a distribui√ß√£o de valores em pagamento_valor_minimo
distribuicao_pagamento_min = df_categoricas["pagamento_valor_minimo"].value_counts(normalize=True) * 100

print("=== Distribui√ß√£o de valores em pagamento_valor_minimo (%) ===")
print(distribuicao_pagamento_min)

# Criar a tabela de conting√™ncia (crosstab) entre pagamento_valor_minimo e comportamento_pagamento
tabela_contingencia = pd.crosstab(
    df_categoricas["pagamento_valor_minimo"],
    df_categoricas["comportamento_pagamento"],
    normalize="columns"  # normaliza por coluna para ver % de Yes/No/NM dentro de cada comportamento
) * 100

# Exibir a tabela com porcentagens arredondadas
print("=== Distribui√ß√£o (%) de Payment_of_Min_Amount por Payment_Behaviour ===")
print(tabela_contingencia.round(2))

# Substituir NM por "Not Informed" para padroniza√ß√£o
df_categoricas["pagamento_valor_minimo"] = df_categoricas["pagamento_valor_minimo"].replace("NM", "Not Informed")

# Conferir resultado
print(df_categoricas["pagamento_valor_minimo"].value_counts())
```

### 3.3. Coluna `comportamento_pagamento`: An√°lise e Tratamento de Valores Inv√°lidos

A coluna `comportamento_pagamento` apresentava um valor inv√°lido `!@9#%8`. As categorias v√°lidas eram coerentes, divididas em `High_spent` (alto gasto) e `Low_spent` (baixo gasto), cada uma subdividida por valor de pagamento (`Small`, `Medium`, `Large`). A presen√ßa do valor inv√°lido sugeria um erro de input que precisava ser investigado.

Para entender o significado desses registros inv√°lidos, foram realizadas duas an√°lises estat√≠sticas complementares utilizando a `renda_anual` como m√©trica comparativa. A primeira an√°lise agrupou os registros em `High_spent`, `Low_spent` e `Invalid` (para os registros `!@9#%8`). A segunda an√°lise refinou essa vis√£o, agrupando os comportamentos em `Small_value_payments`, `Medium_value_payments`, `Large_value_payments` e `Invalid`.

Os resultados mostraram que os registros `Invalid` tinham mediana, m√©dia e desvio-padr√£o da `renda_anual` muito pr√≥ximos aos dos grupos `High_spent` e `Low_spent`, e tamb√©m aos grupos de `value_payments`. Isso indicou que os registros inv√°lidos n√£o se alinhavam a um padr√£o de comportamento de gasto ou valor de pagamento espec√≠fico que justificasse a cria√ß√£o de uma nova categoria ou a imputa√ß√£o em uma categoria existente. Em vez disso, a distribui√ß√£o de renda dos registros inv√°lidos era similar √† distribui√ß√£o geral, sugerindo que o `!@9#%8` era, de fato, um erro de digita√ß√£o ou um valor gen√©rico que n√£o representava um comportamento distinto.

Com base nessa an√°lise, a decis√£o foi remover os registros com o valor `!@9#%8` da coluna `comportamento_pagamento`. A remo√ß√£o foi preferida √† imputa√ß√£o ou cria√ß√£o de uma nova categoria, pois o valor n√£o trazia informa√ß√£o √∫til e sua manuten√ß√£o poderia introduzir ru√≠do no modelo. A quantidade de registros afetados era pequena o suficiente para n√£o impactar significativamente o tamanho do dataset.

```python
# Exibir os 30 primeiros valores √∫nicos
df_categoricas.comportamento_pagamento.unique()[:30]

# Ver a distribui√ß√£o de valores em comportamento_pagamento
distribuicao_comportamento = df_categoricas["comportamento_pagamento"].value_counts(normalize=True) * 100

print("=== Distribui√ß√£o de valores em comportamento_pagamento (%) ===")
print(distribuicao_comportamento)

# Criar uma c√≥pia de trabalho
df_temp = df_processado_type_ok.copy()

# Criar uma coluna indicando apenas "High_spent" ou "Low_spent"
df_temp["grupo_spent"] = df_temp["comportamento_pagamento"].apply(
    lambda x: "High_spent" if "High_spent" in str(x) else ("Low_spent" if "Low_spent" in str(x) else "Invalid")
)

# Calcular estat√≠sticas para High_spent e Low_spent
estatisticas_grupo = df_temp.groupby("grupo_spent")["renda_anual"].agg(
    mediana_renda="median",
    desvio_renda="std",
    media_renda="mean",
    total_registros="count"
)

print("=== Estat√≠sticas de renda anual por grupo_spent ===")
print(estatisticas_grupo)

# Estat√≠sticas s√≥ para os registros inv√°lidos (!@9#%8)
estatisticas_invalidos = df_temp.loc[df_temp["comportamento_pagamento"] == "!@9#%8", "renda_anual"].agg(
    ["median", "std", "mean", "count"]
)

print("\n=== Estat√≠sticas de renda anual para registros inv√°lidos ===")
print(estatisticas_invalidos)

# Criar c√≥pia de trabalho
df_temp = df_processado_type_ok.copy()

# Criar uma coluna 'grupo_valor' com base no padr√£o do texto
def classificar_pagamento(valor):
    if "Small_value_payments" in str(valor):
        return "Small_value_payments"
    elif "Medium_value_payments" in str(valor):
        return "Medium_value_payments"
    elif "Large_value_payments" in str(valor):
        return "Large_value_payments"
    else:
        return "Invalid"

df_temp["grupo_valor"] = df_temp["comportamento_pagamento"].apply(classificar_pagamento)

# Agrupar por grupo_valor e calcular estat√≠sticas de renda anual
estatisticas_valor = df_temp.groupby("grupo_valor")["renda_anual"].agg(
    mediana_renda="median",
    desvio_renda="std",
    media_renda="mean",
    total_registros="count"
).sort_values(by="mediana_renda", ascending=False)

print("=== Estat√≠sticas de renda anual por grupo de valor ===")
print(estatisticas_valor)

# Remover os registros com o valor inv√°lido
df_categoricas = df_categoricas[df_categoricas["comportamento_pagamento"] != "!@9#%8"]

# Conferir se a remo√ß√£o foi feita corretamente
print("\nValores √∫nicos em comportamento_pagamento ap√≥s remo√ß√£o:")
print(df_categoricas["comportamento_pagamento"].unique())
```

### 3.4. Coluna `ocupacao`: Tratamento de Valores Ausentes

A coluna `ocupacao` apresentava valores `_______` que indicavam dados ausentes. Ap√≥s a substitui√ß√£o desses valores por `NaN`, foi observado que aproximadamente 7% dos registros estavam ausentes. A an√°lise da distribui√ß√£o de renda anual para cada ocupa√ß√£o e para os valores ausentes (`_______`) revelou que a mediana, m√©dia e desvio-padr√£o da renda dos registros ausentes eram muito pr√≥ximos aos das demais profiss√µes. Isso sugeriu que a aus√™ncia de informa√ß√£o n√£o estava correlacionada a uma profiss√£o espec√≠fica, mas sim a uma decis√£o do indiv√≠duo de n√£o informar, caracterizando um cen√°rio de Missing Not At Random (MNAR).

Para preservar essa informa√ß√£o e evitar distor√ß√µes na distribui√ß√£o das profiss√µes, a estrat√©gia adotada foi criar uma nova categoria `Not Informed` para os valores ausentes. Essa abordagem √© mais robusta do que a imputa√ß√£o arbitr√°ria, pois mant√©m a integridade dos dados e permite que o modelo aprenda com a aus√™ncia de informa√ß√£o como uma caracter√≠stica em si.

```python
# Exibir os 30 primeiros valores √∫nicos
df_categoricas.ocupacao.unique()[:30]

# 1. Substituir os valores '_______' por NaN
df_categoricas["ocupacao"] = df_categoricas["ocupacao"].replace("_______", np.nan)

# 2. Calcular a porcentagem de cada ocupa√ß√£o
porcentagem_ocupacao = df_categoricas["ocupacao"].value_counts(normalize=True) * 100

# 3. Calcular a porcentagem de valores NaN
porcentagem_nan = df_categoricas["ocupacao"].isna().mean() * 100

# Exibir os resultados
print("=== Porcentagem por ocupa√ß√£o ===")
print(porcentagem_ocupacao)

print("\nPorcentagem de valores NaN:", porcentagem_nan)

# Criar uma c√≥pia
df_temp = df_processado_type_ok.copy()

# Agrupar por profiss√£o e calcular estat√≠sticas de renda anual
estatisticas_ocupacao = df_temp.groupby("ocupacao")["renda_anual"].agg(
    mediana_renda="median",
    desvio_renda="std",
    media_renda="mean",
    total_registros="count"
).sort_values(by="mediana_renda", ascending=False)

# Exibir resultados no output
print(estatisticas_ocupacao)

# Substituir os valores nulos (NaN) por "Not Informed"
df_categoricas["ocupacao"] = df_categoricas["ocupacao"].fillna("Not Informed")

# Conferir se a substitui√ß√£o foi feita corretamente
porcentagem_ocupacao = df_categoricas["ocupacao"].value_counts(normalize=True) * 100
print("=== Porcentagem por ocupa√ß√£o ap√≥s substitui√ß√£o ===")
print(porcentagem_ocupacao)
```

### 3.5. Coluna `tipos_emprestimos`: Padroniza√ß√£o de Categorias

A coluna `tipos_emprestimos` continha diversas categorias que representavam tipos de empr√©stimos. Foi identificada a necessidade de padronizar a categoria `Not Specified` para `Not Informed`, a fim de manter a consist√™ncia na nomenclatura de valores ausentes ou n√£o informados em todo o dataset. Essa padroniza√ß√£o facilita a interpreta√ß√£o e o processamento dos dados, al√©m de evitar a cria√ß√£o de categorias redundantes durante a codifica√ß√£o.

```python
# Exibir os 30 primeiros valores √∫nicos
df_categoricas.tipos_emprestimos.unique()[:30]

# Substituir 'Not Specified' por 'Not Informed' para padroniza√ß√£o
df_categoricas["tipos_emprestimos"] = df_categoricas["tipos_emprestimos"].replace("Not Specified", "Not Informed")

# Conferir resultado
print("=== Distribui√ß√£o de valores em tipos_emprestimos (%) ===")
print(df_categoricas["tipos_emprestimos"].value_counts(normalize=True) * 100)
```

## 4. Feature Engineering e Transforma√ß√µes

Esta se√ß√£o detalha as t√©cnicas de feature engineering e transforma√ß√µes aplicadas √†s vari√°veis num√©ricas do dataset. O objetivo √© otimizar a representa√ß√£o dos dados para melhorar o desempenho dos modelos de Machine Learning, lidando com outliers, assimetrias e a cria√ß√£o de novas features quando aplic√°vel.

### 4.1. Coluna `idade`: Tratamento de Outliers e Discretiza√ß√£o

A coluna `idade` foi analisada para identificar e tratar outliers. A presen√ßa de valores extremos pode distorcer as an√°lises estat√≠sticas e o treinamento de modelos. Ap√≥s a identifica√ß√£o, os outliers foram tratados utilizando a t√©cnica de winsoriza√ß√£o, onde valores abaixo do percentil 1 e acima do percentil 99 foram substitu√≠dos pelos valores desses percentis, respectivamente. Isso ajuda a mitigar o impacto de valores extremos sem remover os dados completamente.

Al√©m do tratamento de outliers, a coluna `idade` foi discretizada em faixas et√°rias. A discretiza√ß√£o transforma uma vari√°vel cont√≠nua em uma vari√°vel categ√≥rica ordinal, o que pode ser ben√©fico para alguns modelos que n√£o lidam bem com a linearidade ou para capturar rela√ß√µes n√£o lineares. As faixas et√°rias foram definidas de forma a criar grupos significativos para a an√°lise de cr√©dito.

```python
# Exibir estat√≠sticas descritivas da coluna idade
print("=== Estat√≠sticas descritivas da coluna idade ===")
print(df_numericas["idade"].describe())

# Identificar e tratar outliers (exemplo com winsoriza√ß√£o)
Q1 = df_numericas["idade"].quantile(0.01)
Q3 = df_numericas["idade"].quantile(0.99)
df_numericas["idade"] = df_numericas["idade"].clip(lower=Q1, upper=Q3)

# Discretiza√ß√£o da coluna idade em faixas et√°rias
bins = [18, 25, 35, 45, 55, 65, 100]
labels = ["18-24", "25-34", "35-44", "45-54", "55-64", "65+"]
df_numericas["faixa_etaria"] = pd.cut(df_numericas["idade"], bins=bins, labels=labels, right=False)

# Conferir a distribui√ß√£o das novas faixas et√°rias
print("\n=== Distribui√ß√£o de faixas et√°rias ===")
print(df_numericas["faixa_etaria"].value_counts(normalize=True) * 100)
```

### 4.2. Coluna `renda_anual`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `renda_anual` apresentava uma distribui√ß√£o altamente assim√©trica e a presen√ßa de outliers extremos, o que √© comum em vari√°veis de renda. Para mitigar o impacto desses outliers e normalizar a distribui√ß√£o, foi aplicada uma transforma√ß√£o logar√≠tmica (log1p, que lida bem com valores zero ou pr√≥ximos de zero). Antes da transforma√ß√£o, os outliers foram tratados com winsoriza√ß√£o, substituindo valores abaixo do percentil 1 e acima do percentil 99 pelos respectivos limites. Essa abordagem ajuda a reduzir a influ√™ncia de valores extremos e a tornar a distribui√ß√£o mais pr√≥xima de uma normal, o que √© ben√©fico para muitos algoritmos de Machine Learning.

```python
# Exibir estat√≠sticas descritivas da coluna renda_anual
print("=== Estat√≠sticas descritivas da coluna renda_anual ===")
print(df_numericas["renda_anual"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["renda_anual"].quantile(0.01)
Q3 = df_numericas["renda_anual"].quantile(0.99)
df_numericas["renda_anual"] = df_numericas["renda_anual"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["renda_anual_log"] = np.log1p(df_numericas["renda_anual"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna renda_anual_log ===")
print(df_numericas["renda_anual_log"].describe())
```

### 4.3. Coluna `salario_liquido_mensal`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

Similar √† `renda_anual`, a coluna `salario_liquido_mensal` tamb√©m exibia assimetria e outliers. O tratamento seguiu a mesma l√≥gica: winsoriza√ß√£o nos percentis 1 e 99 para limitar os valores extremos, seguida pela aplica√ß√£o da transforma√ß√£o logar√≠tmica (log1p). Essa transforma√ß√£o √© eficaz para reduzir a assimetria e estabilizar a vari√¢ncia, tornando a vari√°vel mais adequada para modelos que assumem distribui√ß√µes mais sim√©tricas.

```python
# Exibir estat√≠sticas descritivas da coluna salario_liquido_mensal
print("=== Estat√≠sticas descritivas da coluna salario_liquido_mensal ===")
print(df_numericas["salario_liquido_mensal"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["salario_liquido_mensal"].quantile(0.01)
Q3 = df_numericas["salario_liquido_mensal"].quantile(0.99)
df_numericas["salario_liquido_mensal"] = df_numericas["salario_liquido_mensal"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["salario_liquido_mensal_log"] = np.log1p(df_numericas["salario_liquido_mensal"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna salario_liquido_mensal_log ===")
print(df_numericas["salario_liquido_mensal_log"].describe())
```

### 4.4. Coluna `qtd_contas_bancarias`: Tratamento de Outliers

A coluna `qtd_contas_bancarias` foi analisada para a presen√ßa de outliers. Embora seja uma vari√°vel discreta, valores excessivamente altos podem indicar anomalias ou clientes com perfis muito espec√≠ficos que podem distorcer o modelo. O tratamento de outliers foi realizado por winsoriza√ß√£o nos percentis 1 e 99, garantindo que os valores extremos fossem limitados sem perder a informa√ß√£o da distribui√ß√£o central.

```python
# Exibir estat√≠sticas descritivas da coluna qtd_contas_bancarias
print("=== Estat√≠sticas descritivas da coluna qtd_contas_bancarias ===")
print(df_numericas["qtd_contas_bancarias"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["qtd_contas_bancarias"].quantile(0.01)
Q3 = df_numericas["qtd_contas_bancarias"].quantile(0.99)
df_numericas["qtd_contas_bancarias"] = df_numericas["qtd_contas_bancarias"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna qtd_contas_bancarias ap√≥s tratamento ===")
print(df_numericas["qtd_contas_bancarias"].describe())
```

### 4.5. Coluna `qtd_cartoes_credito`: Tratamento de Outliers

Similar √† `qtd_contas_bancarias`, a coluna `qtd_cartoes_credito` tamb√©m √© uma vari√°vel discreta que pode conter outliers. O tratamento foi realizado com winsoriza√ß√£o nos percentis 1 e 99 para limitar a influ√™ncia de valores extremos, como um n√∫mero excepcionalmente alto ou baixo de cart√µes de cr√©dito, que poderiam ser ru√≠do ou representar casos muito espec√≠ficos que n√£o generalizam bem.

```python
# Exibir estat√≠sticas descritivas da coluna qtd_cartoes_credito
print("=== Estat√≠sticas descritivas da coluna qtd_cartoes_credito ===")
print(df_numericas["qtd_cartoes_credito"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["qtd_cartoes_credito"].quantile(0.01)
Q3 = df_numericas["qtd_cartoes_credito"].quantile(0.99)
df_numericas["qtd_cartoes_credito"] = df_numericas["qtd_cartoes_credito"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna qtd_cartoes_credito ap√≥s tratamento ===")
print(df_numericas["qtd_cartoes_credito"].describe())
```

### 4.6. Coluna `taxa_juros`: Tratamento de Outliers

A coluna `taxa_juros` √© uma vari√°vel num√©rica cont√≠nua que pode apresentar outliers, especialmente taxas de juros muito altas ou muito baixas. O tratamento foi feito por winsoriza√ß√£o nos percentis 1 e 99, a fim de suavizar a influ√™ncia desses valores extremos e garantir que a distribui√ß√£o da vari√°vel seja mais representativa para o treinamento do modelo.

```python
# Exibir estat√≠sticas descritivas da coluna taxa_juros
print("=== Estat√≠sticas descritivas da coluna taxa_juros ===")
print(df_numericas["taxa_juros"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["taxa_juros"].quantile(0.01)
Q3 = df_numericas["taxa_juros"].quantile(0.99)
df_numericas["taxa_juros"] = df_numericas["taxa_juros"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna taxa_juros ap√≥s tratamento ===")
print(df_numericas["taxa_juros"].describe())
```

### 4.7. Coluna `qtd_emprestimos`: Tratamento de Outliers

A coluna `qtd_emprestimos` representa o n√∫mero de empr√©stimos e, como outras vari√°veis de contagem, pode ter outliers. A winsoriza√ß√£o nos percentis 1 e 99 foi aplicada para limitar a influ√™ncia de clientes com um n√∫mero excepcionalmente alto ou baixo de empr√©stimos, garantindo que o modelo n√£o seja excessivamente influenciado por esses casos extremos.

```python
# Exibir estat√≠sticas descritivas da coluna qtd_emprestimos
print("=== Estat√≠sticas descritivas da coluna qtd_emprestimos ===")
print(df_numericas["qtd_emprestimos"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["qtd_emprestimos"].quantile(0.01)
Q3 = df_numericas["qtd_emprestimos"].quantile(0.99)
df_numericas["qtd_emprestimos"] = df_numericas["qtd_emprestimos"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna qtd_emprestimos ap√≥s tratamento ===")
print(df_numericas["qtd_emprestimos"].describe())
```

### 4.8. Coluna `dias_atraso_pagamento`: Tratamento de Outliers

A coluna `dias_atraso_pagamento` indica o n√∫mero de dias de atraso em pagamentos. Valores muito altos nesta coluna s√£o outliers cr√≠ticos, pois representam um comportamento de cr√©dito de alto risco. A winsoriza√ß√£o nos percentis 1 e 99 foi utilizada para limitar esses valores extremos, mantendo a informa√ß√£o de atraso, mas suavizando o impacto dos atrasos mais severos, que poderiam dominar o modelo.

```python
# Exibir estat√≠sticas descritivas da coluna dias_atraso_pagamento
print("=== Estat√≠sticas descritivas da coluna dias_atraso_pagamento ===")
print(df_numericas["dias_atraso_pagamento"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["dias_atraso_pagamento"].quantile(0.01)
Q3 = df_numericas["dias_atraso_pagamento"].quantile(0.99)
df_numericas["dias_atraso_pagamento"] = df_numericas["dias_atraso_pagamento"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna dias_atraso_pagamento ap√≥s tratamento ===")
print(df_numericas["dias_atraso_pagamento"].describe())
```

### 4.9. Coluna `qtd_pagamentos_atrasados`: Tratamento de Outliers

Similar √† coluna de dias de atraso, a `qtd_pagamentos_atrasados` tamb√©m √© uma m√©trica de risco. Outliers nesta coluna (um n√∫mero muito alto de pagamentos atrasados) foram tratados com winsoriza√ß√£o nos percentis 1 e 99. Essa t√©cnica ajuda a manter a representatividade da vari√°vel sem permitir que poucos casos extremos distor√ßam o aprendizado do modelo.

```python
# Exibir estat√≠sticas descritivas da coluna qtd_pagamentos_atrasados
print("=== Estat√≠sticas descritivas da coluna qtd_pagamentos_atrasados ===")
print(df_numericas["qtd_pagamentos_atrasados"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["qtd_pagamentos_atrasados"].quantile(0.01)
Q3 = df_numericas["qtd_pagamentos_atrasados"].quantile(0.99)
df_numericas["qtd_pagamentos_atrasados"] = df_numericas["qtd_pagamentos_atrasados"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna qtd_pagamentos_atrasados ap√≥s tratamento ===")
print(df_numericas["qtd_pagamentos_atrasados"].describe())
```

### 4.10. Coluna `variacao_limite_credito`: Tratamento de Outliers

A coluna `variacao_limite_credito` pode apresentar outliers em ambas as extremidades (varia√ß√µes muito positivas ou muito negativas). A winsoriza√ß√£o nos percentis 1 e 99 foi aplicada para limitar esses valores extremos, garantindo que a vari√°vel contribua de forma mais est√°vel para o modelo, sem ser excessivamente influenciada por mudan√ßas at√≠picas no limite de cr√©dito.

```python
# Exibir estat√≠sticas descritivas da coluna variacao_limite_credito
print("=== Estat√≠sticas descritivas da coluna variacao_limite_credito ===")
print(df_numericas["variacao_limite_credito"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["variacao_limite_credito"].quantile(0.01)
Q3 = df_numericas["variacao_limite_credito"].quantile(0.99)
df_numericas["variacao_limite_credito"] = df_numericas["variacao_limite_credito"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna variacao_limite_credito ap√≥s tratamento ===")
print(df_numericas["variacao_limite_credito"].describe())
```

### 4.11. Coluna `qtd_consultas_credito`: Tratamento de Outliers

A coluna `qtd_consultas_credito` representa o n√∫mero de consultas de cr√©dito. Um n√∫mero excessivamente alto de consultas pode indicar um comportamento de busca de cr√©dito arriscado. Outliers nesta coluna foram tratados com winsoriza√ß√£o nos percentis 1 e 99, para mitigar o impacto de valores extremos e garantir que a vari√°vel seja mais robusta para o modelo.

```python
# Exibir estat√≠sticas descritivas da coluna qtd_consultas_credito
print("=== Estat√≠sticas descritivas da coluna qtd_consultas_credito ===")
print(df_numericas["qtd_consultas_credito"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["qtd_consultas_credito"].quantile(0.01)
Q3 = df_numericas["qtd_consultas_credito"].quantile(0.99)
df_numericas["qtd_consultas_credito"] = df_numericas["qtd_consultas_credito"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna qtd_consultas_credito ap√≥s tratamento ===")
print(df_numericas["qtd_consultas_credito"].describe())
```

### 4.12. Coluna `divida_pendente`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `divida_pendente` √© uma vari√°vel financeira que frequentemente apresenta assimetria e outliers. O tratamento incluiu a winsoriza√ß√£o nos percentis 1 e 99 para limitar os valores extremos, seguida pela aplica√ß√£o da transforma√ß√£o logar√≠tmica (log1p). Essa combina√ß√£o ajuda a normalizar a distribui√ß√£o da d√≠vida pendente, tornando-a mais adequada para modelos que assumem distribui√ß√µes mais sim√©tricas e reduzindo a influ√™ncia de d√≠vidas excepcionalmente altas.

```python
# Exibir estat√≠sticas descritivas da coluna divida_pendente
print("=== Estat√≠sticas descritivas da coluna divida_pendente ===")
print(df_numericas["divida_pendente"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["divida_pendente"].quantile(0.01)
Q3 = df_numericas["divida_pendente"].quantile(0.99)
df_numericas["divida_pendente"] = df_numericas["divida_pendente"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["divida_pendente_log"] = np.log1p(df_numericas["divida_pendente"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna divida_pendente_log ===")
print(df_numericas["divida_pendente_log"].describe())
```

### 4.13. Coluna `percentual_utilizacao_credito`: Tratamento de Outliers

A coluna `percentual_utilizacao_credito` indica o qu√£o pr√≥ximo o cliente est√° do seu limite de cr√©dito. Valores muito altos (pr√≥ximos a 100%) ou muito baixos podem ser outliers. A winsoriza√ß√£o nos percentis 1 e 99 foi aplicada para limitar esses valores extremos, garantindo que a vari√°vel seja mais representativa do comportamento geral de utiliza√ß√£o de cr√©dito e menos suscet√≠vel a casos at√≠picos.

```python
# Exibir estat√≠sticas descritivas da coluna percentual_utilizacao_credito
print("=== Estat√≠sticas descritivas da coluna percentual_utilizacao_credito ===")
print(df_numericas["percentual_utilizacao_credito"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["percentual_utilizacao_credito"].quantile(0.01)
Q3 = df_numericas["percentual_utilizacao_credito"].quantile(0.99)
df_numericas["percentual_utilizacao_credito"] = df_numericas["percentual_utilizacao_credito"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna percentual_utilizacao_credito ap√≥s tratamento ===")
print(df_numericas["percentual_utilizacao_credito"].describe())
```

### 4.14. Coluna `total_emprestimos_mensal`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `total_emprestimos_mensal` (total de empr√©stimos mensais) √© outra vari√°vel financeira que tende a ter uma distribui√ß√£o assim√©trica e outliers. O tratamento envolveu a winsoriza√ß√£o nos percentis 1 e 99, seguida pela transforma√ß√£o logar√≠tmica (log1p). Essa abordagem padroniza a vari√°vel, reduzindo a assimetria e a influ√™ncia de valores extremos, o que √© fundamental para a estabilidade do modelo.

```python
# Exibir estat√≠sticas descritivas da coluna total_emprestimos_mensal
print("=== Estat√≠sticas descritivas da coluna total_emprestimos_mensal ===")
print(df_numericas["total_emprestimos_mensal"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["total_emprestimos_mensal"].quantile(0.01)
Q3 = df_numericas["total_emprestimos_mensal"].quantile(0.99)
df_numericas["total_emprestimos_mensal"] = df_numericas["total_emprestimos_mensal"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["total_emprestimos_mensal_log"] = np.log1p(df_numericas["total_emprestimos_mensal"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna total_emprestimos_mensal_log ===")
print(df_numericas["total_emprestimos_mensal_log"].describe())
```

### 4.15. Coluna `valor_investido_mensal`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `valor_investido_mensal` (valor investido mensalmente) tamb√©m √© uma vari√°vel financeira que pode apresentar uma distribui√ß√£o assim√©trica e outliers. O tratamento foi realizado com winsoriza√ß√£o nos percentis 1 e 99, seguida pela transforma√ß√£o logar√≠tmica (log1p). Essa estrat√©gia visa normalizar a distribui√ß√£o e reduzir a influ√™ncia de valores de investimento excepcionalmente altos, que poderiam enviesar o modelo.

```python
# Exibir estat√≠sticas descritivas da coluna valor_investido_mensal
print("=== Estat√≠sticas descritivas da coluna valor_investido_mensal ===")
print(df_numericas["valor_investido_mensal"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["valor_investido_mensal"].quantile(0.01)
Q3 = df_numericas["valor_investido_mensal"].quantile(0.99)
df_numericas["valor_investido_mensal"] = df_numericas["valor_investido_mensal"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["valor_investido_mensal_log"] = np.log1p(df_numericas["valor_investido_mensal"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna valor_investido_mensal_log ===")
print(df_numericas["valor_investido_mensal_log"].describe())
```

### 4.16. Coluna `saldo_mensal`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `saldo_mensal` (saldo mensal) √© outra vari√°vel financeira que pode se beneficiar do tratamento de outliers e da transforma√ß√£o logar√≠tmica. A winsoriza√ß√£o nos percentis 1 e 99 foi aplicada para limitar os valores extremos, seguida pela transforma√ß√£o logar√≠tmica (log1p). Essa abordagem ajuda a estabilizar a vari√¢ncia e a normalizar a distribui√ß√£o, tornando a vari√°vel mais adequada para o treinamento do modelo.

```python
# Exibir estat√≠sticas descritivas da coluna saldo_mensal
print("=== Estat√≠sticas descritivas da coluna saldo_mensal ===")
print(df_numericas["saldo_mensal"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["saldo_mensal"].quantile(0.01)
Q3 = df_numericas["saldo_mensal"].quantile(0.99)
df_numericas["saldo_mensal"] = df_numericas["saldo_mensal"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["saldo_mensal_log"] = np.log1p(df_numericas["saldo_mensal"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna saldo_mensal_log ===")
print(df_numericas["saldo_mensal_log"].describe())
```

### 4.17. Coluna `tempo_historico_credito_meses`: Tratamento de Outliers

A coluna `tempo_historico_credito_meses` (tempo de hist√≥rico de cr√©dito em meses) √© uma vari√°vel importante para a avalia√ß√£o de cr√©dito. Outliers nesta coluna (tempos de hist√≥rico muito curtos ou muito longos) foram tratados com winsoriza√ß√£o nos percentis 1 e 99. Essa t√©cnica ajuda a garantir que a vari√°vel seja mais robusta e menos suscet√≠vel a valores extremos que poderiam distorcer a an√°lise de cr√©dito.

```python
# Exibir estat√≠sticas descritivas da coluna tempo_historico_credito_meses
print("=== Estat√≠sticas descritivas da coluna tempo_historico_credito_meses ===")
print(df_numericas["tempo_historico_credito_meses"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["tempo_historico_credito_meses"].quantile(0.01)
Q3 = df_numericas["tempo_historico_credito_meses"].quantile(0.99)
df_numericas["tempo_historico_credito_meses"] = df_numericas["tempo_historico_credito_meses"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna tempo_historico_credito_meses ap√≥s tratamento ===")
print(df_numericas["tempo_historico_credito_meses"].describe())
```

## 5. An√°lise de Correla√ß√£o e Multicolinearidade

Esta se√ß√£o descreve a an√°lise de correla√ß√£o entre as vari√°veis num√©ricas e o target, bem como a investiga√ß√£o de multicolinearidade entre as features, utilizando o VIF (Variance Inflation Factor).

### 5.1. Correla√ß√£o com o Target (Pearson e Spearman)

Foram calculadas as correla√ß√µes de Pearson e Spearman entre as vari√°veis num√©ricas e o target (`score_credito_num`). A correla√ß√£o de Pearson mede a rela√ß√£o linear, enquanto a de Spearman mede a rela√ß√£o monot√¥nica, sendo mais robusta a outliers e distribui√ß√µes n√£o normais. A an√°lise revelou que nenhuma vari√°vel apresentava uma correla√ß√£o extremamente alta com o target, o que √© um bom indicativo para evitar *data leakage* e garantir que o modelo precise de m√∫ltiplas features para fazer suas previs√µes. Os resultados foram ordenados pelo valor absoluto da correla√ß√£o para identificar as vari√°veis mais influentes.

```python
# Calcula a correla√ß√£o de Pearson
corr_pearson = df_numericas_e_maptarget.corr(method="pearson")

# Ordena as correla√ß√µes do target pelo valor absoluto, mas preserva o sinal
corr_target_pearson = corr_pearson["score_credito_num"]
corr_target_pearson = corr_target_pearson.reindex(corr_target_pearson.abs().sort_values(ascending=False).index)

print("Correla√ß√£o do target (score_credito_num) com as vari√°veis (Pearson - ordenada pelo valor absoluto):\n")
print(corr_target_pearson)

# Calcula a correla√ß√£o de Spearman
corr_spearman = df_numericas_e_maptarget.corr(method="spearman")

# Ordena as correla√ß√µes do target pelo valor absoluto, mas preserva o sinal
corr_target_spearman = corr_spearman["score_credito_num"]
corr_target_spearman = corr_target_spearman.reindex(corr_target_spearman.abs().sort_values(ascending=False).index)

print("\nCorrela√ß√£o do target (score_credito_num) com as vari√°veis (Spearman - ordenada pelo valor absoluto):\n")
print(corr_target_spearman)
```

### 5.2. An√°lise de VIF (Variance Inflation Factor)

Para investigar a multicolinearidade entre as vari√°veis num√©ricas, foi calculado o VIF (Variance Inflation Factor). O VIF mede o quanto a vari√¢ncia de um coeficiente de regress√£o estimado √© inflacionada devido √† multicolinearidade. Valores de VIF acima de 5 ou 10 geralmente indicam problemas. A an√°lise mostrou que todas as vari√°veis apresentavam valores de VIF entre 1.00 e 2.50, o que √© considerado um n√≠vel saud√°vel de correla√ß√£o e muito abaixo dos limites problem√°ticos. Mesmo a alta correla√ß√£o observada entre `renda_anual` e `salario_liquido_mensal` n√£o resultou em um VIF elevado, indicando que ambas as vari√°veis poderiam ser mantidas sem causar problemas significativos de multicolinearidade no modelo. Diante disso, decidiu-se n√£o remover nenhuma vari√°vel neste momento, deixando eventuais ajustes de sele√ß√£o de vari√°veis para serem avaliados ap√≥s os primeiros testes de modelagem.

```python
from sklearn.linear_model import LinearRegression
import numpy as np

def calcular_vif(df):
    """
    Calcula o VIF (Variance Inflation Factor) manualmente para cada coluna do DataFrame.
    """
    vif_dict = {}
    X = df.values

    for i in range(df.shape[1]):
        y = X[:, i]  # vari√°vel alvo (uma coluna)
        X_outras = np.delete(X, i, axis=1)  # todas as outras vari√°veis

        modelo = LinearRegression()
        modelo.fit(X_outras, y)
        r2 = modelo.score(X_outras, y)

        vif = 1 / (1 - r2)
        vif_dict[df.columns[i]] = vif

    return pd.DataFrame({"Vari√°vel": list(vif_dict.keys()), "VIF": list(vif_dict.values())}).sort_values(by="VIF", ascending=False)

# Executa o c√°lculo de VIF nas vari√°veis num√©ricas
vif_df = calcular_vif(df_numericas)

print("üìä Fatores de Infla√ß√£o de Vari√¢ncia (VIF) - Calculados sem statsmodels:\")
print(vif_df)
```

## 6. Codifica√ß√£o e Exporta√ß√£o Final

Esta se√ß√£o finaliza o pr√©-processamento dos dados, abordando o mapeamento da vari√°vel target e a codifica√ß√£o das vari√°veis categ√≥ricas, culminando na exporta√ß√£o do dataset final pronto para a fase de modelagem.

### 6.1. Mapeamento do Target (`score_credito`)

Antes de aplicar o One-Hot Encoding nas vari√°veis categ√≥ricas, a vari√°vel target `score_credito` foi mapeada para valores num√©ricos ordinais: `Poor = 0`, `Standard = 1` e `Good = 2`. Essa etapa √© crucial para preparar o target para algoritmos de Machine Learning, que geralmente exigem entradas num√©ricas. Ao mapear o target separadamente, evitamos que ele seja erroneamente codificado junto com as features categ√≥ricas, mantendo uma distin√ß√£o clara entre features e target. O dataset final j√° ter√° o target no formato correto, facilitando a divis√£o em conjuntos de treino e teste na etapa de modelagem.

```python
# Faz uma c√≥pia do df_categoricas para n√£o alterar o original
df_categoricas_mapeado = df_categoricas.copy()

# Mapeia a coluna score_credito diretamente
mapa_target = {"Poor": 0, "Standard": 1, "Good": 2}
df_categoricas_mapeado["score_credito"] = df_categoricas_mapeado["score_credito"].map(mapa_target)

# Confere o resultado
df_categoricas_mapeado[["score_credito"]].head()
```

### 6.2. One-Hot Encoding das Vari√°veis Categ√≥ricas

As vari√°veis categ√≥ricas restantes (exceto o target j√° mapeado) foram codificadas utilizando a t√©cnica de One-Hot Encoding. Esta t√©cnica transforma cada categoria em uma nova coluna bin√°ria (0 ou 1), o que √© essencial para que algoritmos de Machine Learning possam processar vari√°veis categ√≥ricas. A op√ß√£o `drop_first=True` foi utilizada para evitar a armadilha da multicolinearidade, removendo uma das categorias de cada vari√°vel original. Isso garante que o modelo n√£o seja prejudicado por vari√°veis linearmente dependentes.

```python
# Faz uma c√≥pia do df_categoricas_mapeado
df_categoricas_get_dummies = df_categoricas_mapeado.copy()

# Aplica get_dummies em todas as colunas categ√≥ricas, exceto o target
df_categoricas_get_dummies = pd.get_dummies(
    df_categoricas_get_dummies.drop(columns=["score_credito"]), 
    drop_first=True
)

# Confere as primeiras linhas
df_categoricas_get_dummies.head()
```

### 6.3. Concatena√ß√£o e Exporta√ß√£o do Dataset Final

Finalmente, as vari√°veis num√©ricas (j√° tratadas e transformadas) e as vari√°veis categ√≥ricas (j√° codificadas) foram concatenadas para formar o dataset final. A vari√°vel target (`score_credito`), j√° mapeada numericamente, foi adicionada como a √∫ltima coluna. O DataFrame resultante, `df_final`, est√° agora completamente pr√©-processado e pronto para ser utilizado na fase de modelagem. Este dataset foi exportado para um arquivo CSV, garantindo que todas as transforma√ß√µes e tratamentos sejam persistidos e possam ser facilmente carregados para o treinamento de modelos.

```python
# Concatena num√©ricas e categ√≥ricas codificadas
df_final = pd.concat([df_numericas, df_categoricas_get_dummies], axis=1)

# Adiciona o target (j√° mapeado) como √∫ltima coluna
df_final["score_credito"] = df_categoricas_mapeado["score_credito"]

# Conferir o DataFrame final
df_final.info()

# Exibir as primeiras linhas do DataFrame final
df_final.head()

# Exportar o DataFrame final para um arquivo CSV
df_final.to_csv("df_processado_final.csv", index=False)
```

## 7. Conclus√£o

Este documento detalhou exaustivamente as etapas de pr√©-processamento de dados, tratamento de valores ausentes e inconsist√™ncias, feature engineering e an√°lise de multicolinearidade realizadas no projeto de previs√£o de score de cr√©dito. Cada decis√£o foi justificada com base em an√°lises estat√≠sticas e melhores pr√°ticas de Machine Learning, visando a constru√ß√£o de um modelo robusto e confi√°vel. O dataset final, `df_processado_final.csv`, est√° agora preparado para a fase de modelagem, onde diferentes algoritmos poder√£o ser aplicados e avaliados para prever o score de cr√©dito com alta precis√£o e interpretabilidade. A documenta√ß√£o serve como um guia completo para futuras itera√ß√µes e para garantir a reprodutibilidade do pipeline de dados.

### 4.12. Coluna `divida_pendente`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `divida_pendente` √© uma vari√°vel financeira que frequentemente apresenta assimetria e outliers. O tratamento incluiu a winsoriza√ß√£o nos percentis 1 e 99 para limitar os valores extremos, seguida pela aplica√ß√£o da transforma√ß√£o logar√≠tmica (log1p). Essa combina√ß√£o ajuda a normalizar a distribui√ß√£o da d√≠vida pendente, tornando-a mais adequada para modelos que assumem distribui√ß√µes mais sim√©tricas e reduzindo a influ√™ncia de d√≠vidas excepcionalmente altas.

```python
# Exibir estat√≠sticas descritivas da coluna divida_pendente
print("=== Estat√≠sticas descritivas da coluna divida_pendente ===")
print(df_numericas["divida_pendente"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["divida_pendente"].quantile(0.01)
Q3 = df_numericas["divida_pendente"].quantile(0.99)
df_numericas["divida_pendente"] = df_numericas["divida_pendente"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["divida_pendente_log"] = np.log1p(df_numericas["divida_pendente"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna divida_pendente_log ===")
print(df_numericas["divida_pendente_log"].describe())
```

### 4.13. Coluna `percentual_utilizacao_credito`: Tratamento de Outliers

A coluna `percentual_utilizacao_credito` indica o qu√£o pr√≥ximo o cliente est√° do seu limite de cr√©dito. Valores muito altos (pr√≥ximos a 100%) ou muito baixos podem ser outliers. A winsoriza√ß√£o nos percentis 1 e 99 foi aplicada para limitar esses valores extremos, garantindo que a vari√°vel seja mais representativa do comportamento geral de utiliza√ß√£o de cr√©dito e menos suscet√≠vel a casos at√≠picos.

```python
# Exibir estat√≠sticas descritivas da coluna percentual_utilizacao_credito
print("=== Estat√≠sticas descritivas da coluna percentual_utilizacao_credito ===")
print(df_numericas["percentual_utilizacao_credito"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["percentual_utilizacao_credito"].quantile(0.01)
Q3 = df_numericas["percentual_utilizacao_credito"].quantile(0.99)
df_numericas["percentual_utilizacao_credito"] = df_numericas["percentual_utilizacao_credito"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna percentual_utilizacao_credito ap√≥s tratamento ===")
print(df_numericas["percentual_utilizacao_credito"].describe())
```

### 4.14. Coluna `total_emprestimos_mensal`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `total_emprestimos_mensal` (total de empr√©stimos mensais) √© outra vari√°vel financeira que tende a ter uma distribui√ß√£o assim√©trica e outliers. O tratamento envolveu a winsoriza√ß√£o nos percentis 1 e 99, seguida pela transforma√ß√£o logar√≠tmica (log1p). Essa abordagem padroniza a vari√°vel, reduzindo a assimetria e a influ√™ncia de valores extremos, o que √© fundamental para a estabilidade do modelo.

```python
# Exibir estat√≠sticas descritivas da coluna total_emprestimos_mensal
print("=== Estat√≠sticas descritivas da coluna total_emprestimos_mensal ===")
print(df_numericas["total_emprestimos_mensal"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["total_emprestimos_mensal"].quantile(0.01)
Q3 = df_numericas["total_emprestimos_mensal"].quantile(0.99)
df_numericas["total_emprestimos_mensal"] = df_numericas["total_emprestimos_mensal"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["total_emprestimos_mensal_log"] = np.log1p(df_numericas["total_emprestimos_mensal"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna total_emprestimos_mensal_log ===")
print(df_numericas["total_emprestimos_mensal_log"].describe())
```

### 4.15. Coluna `valor_investido_mensal`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `valor_investido_mensal` (valor investido mensalmente) tamb√©m √© uma vari√°vel financeira que pode apresentar uma distribui√ß√£o assim√©trica e outliers. O tratamento foi realizado com winsoriza√ß√£o nos percentis 1 e 99, seguida pela transforma√ß√£o logar√≠tmica (log1p). Essa estrat√©gia visa normalizar a distribui√ß√£o e reduzir a influ√™ncia de valores de investimento excepcionalmente altos, que poderiam enviesar o modelo.

```python
# Exibir estat√≠sticas descritivas da coluna valor_investido_mensal
print("=== Estat√≠sticas descritivas da coluna valor_investido_mensal ===")
print(df_numericas["valor_investido_mensal"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["valor_investido_mensal"].quantile(0.01)
Q3 = df_numericas["valor_investido_mensal"].quantile(0.99)
df_numericas["valor_investido_mensal"] = df_numericas["valor_investido_mensal"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["valor_investido_mensal_log"] = np.log1p(df_numericas["valor_investido_mensal"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna valor_investido_mensal_log ===")
print(df_numericas["valor_investido_mensal_log"].describe())
```

### 4.16. Coluna `saldo_mensal`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `saldo_mensal` (saldo mensal) √© outra vari√°vel financeira que pode se beneficiar do tratamento de outliers e da transforma√ß√£o logar√≠tmica. A winsoriza√ß√£o nos percentis 1 e 99 foi aplicada para limitar os valores extremos, seguida pela transforma√ß√£o logar√≠tmica (log1p). Essa abordagem ajuda a estabilizar a vari√¢ncia e a normalizar a distribui√ß√£o, tornando a vari√°vel mais adequada para o treinamento do modelo.

```python
# Exibir estat√≠sticas descritivas da coluna saldo_mensal
print("=== Estat√≠sticas descritivas da coluna saldo_mensal ===")
print(df_numericas["saldo_mensal"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["saldo_mensal"].quantile(0.01)
Q3 = df_numericas["saldo_mensal"].quantile(0.99)
df_numericas["saldo_mensal"] = df_numericas["saldo_mensal"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["saldo_mensal_log"] = np.log1p(df_numericas["saldo_mensal"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna saldo_mensal_log ===")
print(df_numericas["saldo_mensal_log"].describe())
```

### 4.17. Coluna `tempo_historico_credito_meses`: Tratamento de Outliers

A coluna `tempo_historico_credito_meses` (tempo de hist√≥rico de cr√©dito em meses) √© uma vari√°vel importante para a avalia√ß√£o de cr√©dito. Outliers nesta coluna (tempos de hist√≥rico muito curtos ou muito longos) foram tratados com winsoriza√ß√£o nos percentis 1 e 99. Essa t√©cnica ajuda a garantir que a vari√°vel seja mais robusta e menos suscet√≠vel a valores extremos que poderiam distorcer a an√°lise de cr√©dito.

```python
# Exibir estat√≠sticas descritivas da coluna tempo_historico_credito_meses
print("=== Estat√≠sticas descritivas da coluna tempo_historico_credito_meses ===")
print(df_numericas["tempo_historico_credito_meses"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["tempo_historico_credito_meses"].quantile(0.01)
Q3 = df_numericas["tempo_historico_credito_meses"].quantile(0.99)
df_numericas["tempo_historico_credito_meses"] = df_numericas["tempo_historico_credito_meses"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna tempo_historico_credito_meses ap√≥s tratamento ===")
print(df_numericas["tempo_historico_credito_meses"].describe())
```

## 5. An√°lise de Correla√ß√£o e Multicolinearidade

Esta se√ß√£o descreve a an√°lise de correla√ß√£o entre as vari√°veis num√©ricas e o target, bem como a investiga√ß√£o de multicolinearidade entre as features, utilizando o VIF (Variance Inflation Factor).

### 5.1. Correla√ß√£o com o Target (Pearson e Spearman)

Foram calculadas as correla√ß√µes de Pearson e Spearman entre as vari√°veis num√©ricas e o target (`score_credito_num`). A correla√ß√£o de Pearson mede a rela√ß√£o linear, enquanto a de Spearman mede a rela√ß√£o monot√¥nica, sendo mais robusta a outliers e distribui√ß√µes n√£o normais. A an√°lise revelou que nenhuma vari√°vel apresentava uma correla√ß√£o extremamente alta com o target, o que √© um bom indicativo para evitar *data leakage* e garantir que o modelo precise de m√∫ltiplas features para fazer suas previs√µes. Os resultados foram ordenados pelo valor absoluto da correla√ß√£o para identificar as vari√°veis mais influentes.

```python
# Calcula a correla√ß√£o de Pearson
corr_pearson = df_numericas_e_maptarget.corr(method="pearson")

# Ordena as correla√ß√µes do target pelo valor absoluto, mas preserva o sinal
corr_target_pearson = corr_pearson["score_credito_num"]
corr_target_pearson = corr_target_pearson.reindex(corr_target_pearson.abs().sort_values(ascending=False).index)

print("Correla√ß√£o do target (score_credito_num) com as vari√°veis (Pearson - ordenada pelo valor absoluto):\n")
print(corr_target_pearson)

# Calcula a correla√ß√£o de Spearman
corr_spearman = df_numericas_e_maptarget.corr(method="spearman")

# Ordena as correla√ß√µes do target pelo valor absoluto, mas preserva o sinal
corr_target_spearman = corr_spearman["score_credito_num"]
corr_target_spearman = corr_target_spearman.reindex(corr_target_spearman.abs().sort_values(ascending=False).index)

print("\nCorrela√ß√£o do target (score_credito_num) com as vari√°veis (Spearman - ordenada pelo valor absoluto):\n")
print(corr_target_spearman)
```

### 5.2. An√°lise de VIF (Variance Inflation Factor)

Para investigar a multicolinearidade entre as vari√°veis num√©ricas, foi calculado o VIF (Variance Inflation Factor). O VIF mede o quanto a vari√¢ncia de um coeficiente de regress√£o estimado √© inflacionada devido √† multicolinearidade. Valores de VIF acima de 5 ou 10 geralmente indicam problemas. A an√°lise mostrou que todas as vari√°veis apresentavam valores de VIF entre 1.00 e 2.50, o que √© considerado um n√≠vel saud√°vel de correla√ß√£o e muito abaixo dos limites problem√°ticos. Mesmo a alta correla√ß√£o observada entre `renda_anual` e `salario_liquido_mensal` n√£o resultou em um VIF elevado, indicando que ambas as vari√°veis poderiam ser mantidas sem causar problemas significativos de multicolinearidade no modelo. Diante disso, decidiu-se n√£o remover nenhuma vari√°vel neste momento, deixando eventuais ajustes de sele√ß√£o de vari√°veis para serem avaliados ap√≥s os primeiros testes de modelagem.

```python
from sklearn.linear_model import LinearRegression
import numpy as np

def calcular_vif(df):
    """
    Calcula o VIF (Variance Inflation Factor) manualmente para cada coluna do DataFrame.
    """
    vif_dict = {}
    X = df.values

    for i in range(df.shape[1]):
        y = X[:, i]  # vari√°vel alvo (uma coluna)
        X_outras = np.delete(X, i, axis=1)  # todas as outras vari√°veis

        modelo = LinearRegression()
        modelo.fit(X_outras, y)
        r2 = modelo.score(X_outras, y)

        vif = 1 / (1 - r2)
        vif_dict[df.columns[i]] = vif

    return pd.DataFrame({"Vari√°vel": list(vif_dict.keys()), "VIF": list(vif_dict.values())}).sort_values(by="VIF", ascending=False)

# Executa o c√°lculo de VIF nas vari√°veis num√©ricas
vif_df = calcular_vif(df_numericas)

print("üìä Fatores de Infla√ß√£o de Vari√¢ncia (VIF) - Calculados sem statsmodels:\")
print(vif_df)
```

## 6. Codifica√ß√£o e Exporta√ß√£o Final

Esta se√ß√£o finaliza o pr√©-processamento dos dados, abordando o mapeamento da vari√°vel target e a codifica√ß√£o das vari√°veis categ√≥ricas, culminando na exporta√ß√£o do dataset final pronto para a fase de modelagem.

### 6.1. Mapeamento do Target (`score_credito`)

Antes de aplicar o One-Hot Encoding nas vari√°veis categ√≥ricas, a vari√°vel target `score_credito` foi mapeada para valores num√©ricos ordinais: `Poor = 0`, `Standard = 1` e `Good = 2`. Essa etapa √© crucial para preparar o target para algoritmos de Machine Learning, que geralmente exigem entradas num√©ricas. Ao mapear o target separadamente, evitamos que ele seja erroneamente codificado junto com as features categ√≥ricas, mantendo uma distin√ß√£o clara entre features e target. O dataset final j√° ter√° o target no formato correto, facilitando a divis√£o em conjuntos de treino e teste na etapa de modelagem.

```python
# Faz uma c√≥pia do df_categoricas para n√£o alterar o original
df_categoricas_mapeado = df_categoricas.copy()

# Mapeia a coluna score_credito diretamente
mapa_target = {"Poor": 0, "Standard": 1, "Good": 2}
df_categoricas_mapeado["score_credito"] = df_categoricas_mapeado["score_credito"].map(mapa_target)

# Confere o resultado
df_categoricas_mapeado[["score_credito"]].head()
```

### 6.2. One-Hot Encoding das Vari√°veis Categ√≥ricas

As vari√°veis categ√≥ricas restantes (exceto o target j√° mapeado) foram codificadas utilizando a t√©cnica de One-Hot Encoding. Esta t√©cnica transforma cada categoria em uma nova coluna bin√°ria (0 ou 1), o que √© essencial para que algoritmos de Machine Learning possam processar vari√°veis categ√≥ricas. A op√ß√£o `drop_first=True` foi utilizada para evitar a armadilha da multicolinearidade, removendo uma das categorias de cada vari√°vel original. Isso garante que o modelo n√£o seja prejudicado por vari√°veis linearmente dependentes.

```python
# Faz uma c√≥pia do df_categoricas_mapeado
df_categoricas_get_dummies = df_categoricas_mapeado.copy()

# Aplica get_dummies em todas as colunas categ√≥ricas, exceto o target
df_categoricas_get_dummies = pd.get_dummies(
    df_categoricas_get_dummies.drop(columns=["score_credito"]), 
    drop_first=True
)

# Confere as primeiras linhas
df_categoricas_get_dummies.head()
```

### 6.3. Concatena√ß√£o e Exporta√ß√£o do Dataset Final

Finalmente, as vari√°veis num√©ricas (j√° tratadas e transformadas) e as vari√°veis categ√≥ricas (j√° codificadas) foram concatenadas para formar o dataset final. A vari√°vel target (`score_credito`), j√° mapeada numericamente, foi adicionada como a √∫ltima coluna. O DataFrame resultante, `df_final`, est√° agora completamente pr√©-processado e pronto para ser utilizado na fase de modelagem. Este dataset foi exportado para um arquivo CSV, garantindo que todas as transforma√ß√µes e tratamentos sejam persistidos e possam ser facilmente carregados para o treinamento de modelos.

```python
# Concatena num√©ricas e categ√≥ricas codificadas
df_final = pd.concat([df_numericas, df_categoricas_get_dummies], axis=1)

# Adiciona o target (j√° mapeado) como √∫ltima coluna
df_final["score_credito"] = df_categoricas_mapeado["score_credito"]

# Conferir o DataFrame final
df_final.info()

# Exibir as primeiras linhas do DataFrame final
df_final.head()

# Exportar o DataFrame final para um arquivo CSV
df_final.to_csv("df_processado_final.csv", index=False)
```

## 7. Conclus√£o

Este documento detalhou exaustivamente as etapas de pr√©-processamento de dados, tratamento de valores ausentes e inconsist√™ncias, feature engineering e an√°lise de multicolinearidade realizadas no projeto de previs√£o de score de cr√©dito. Cada decis√£o foi justificada com base em an√°lises estat√≠sticas e melhores pr√°ticas de Machine Learning, visando a constru√ß√£o de um modelo robusto e confi√°vel. O dataset final, `df_processado_final.csv`, est√° agora preparado para a fase de modelagem, onde diferentes algoritmos poder√£o ser aplicados e avaliados para prever o score de cr√©dito com alta precis√£o e interpretabilidade. A documenta√ß√£o serve como um guia completo para futuras itera√ß√µes e para garantir a reprodutibilidade do pipeline de dados.

## 5. An√°lise de Correla√ß√£o e Multicolinearidade

Esta se√ß√£o descreve a an√°lise de correla√ß√£o entre as vari√°veis num√©ricas e o target, bem como a investiga√ß√£o de multicolinearidade entre as features, utilizando o VIF (Variance Inflation Factor).

### 5.1. Correla√ß√£o com o Target (Pearson e Spearman)

Foram calculadas as correla√ß√µes de Pearson e Spearman entre as vari√°veis num√©ricas e o target (`score_credito_num`). A correla√ß√£o de Pearson mede a rela√ß√£o linear, enquanto a de Spearman mede a rela√ß√£o monot√¥nica, sendo mais robusta a outliers e distribui√ß√µes n√£o normais. A an√°lise revelou que nenhuma vari√°vel apresentava uma correla√ß√£o extremamente alta com o target, o que √© um bom indicativo para evitar *data leakage* e garantir que o modelo precise de m√∫ltiplas features para fazer suas previs√µes. Os resultados foram ordenados pelo valor absoluto da correla√ß√£o para identificar as vari√°veis mais influentes.

```python
# Calcula a correla√ß√£o de Pearson
corr_pearson = df_numericas_e_maptarget.corr(method="pearson")

# Ordena as correla√ß√µes do target pelo valor absoluto, mas preserva o sinal
corr_target_pearson = corr_pearson["score_credito_num"]
corr_target_pearson = corr_target_pearson.reindex(corr_target_pearson.abs().sort_values(ascending=False).index)

print("Correla√ß√£o do target (score_credito_num) com as vari√°veis (Pearson - ordenada pelo valor absoluto):\n")
print(corr_target_pearson)

# Calcula a correla√ß√£o de Spearman
corr_spearman = df_numericas_e_maptarget.corr(method="spearman")

# Ordena as correla√ß√µes do target pelo valor absoluto, mas preserva o sinal
corr_target_spearman = corr_spearman["score_credito_num"]
corr_target_spearman = corr_target_spearman.reindex(corr_target_spearman.abs().sort_values(ascending=False).index)

print("\nCorrela√ß√£o do target (score_credito_num) com as vari√°veis (Spearman - ordenada pelo valor absoluto):\n")
print(corr_target_spearman)
```

### 5.2. An√°lise de VIF (Variance Inflation Factor)

Para investigar a multicolinearidade entre as vari√°veis num√©ricas, foi calculado o VIF (Variance Inflation Factor). O VIF mede o quanto a vari√¢ncia de um coeficiente de regress√£o estimado √© inflacionada devido √† multicolinearidade. Valores de VIF acima de 5 ou 10 geralmente indicam problemas. A an√°lise mostrou que todas as vari√°veis apresentavam valores de VIF entre 1.00 e 2.50, o que √© considerado um n√≠vel saud√°vel de correla√ß√£o e muito abaixo dos limites problem√°ticos. Mesmo a alta correla√ß√£o observada entre `renda_anual` e `salario_liquido_mensal` n√£o resultou em um VIF elevado, indicando que ambas as vari√°veis poderiam ser mantidas sem causar problemas significativos de multicolinearidade no modelo. Diante disso, decidiu-se n√£o remover nenhuma vari√°vel neste momento, deixando eventuais ajustes de sele√ß√£o de vari√°veis para serem avaliados ap√≥s os primeiros testes de modelagem.

```python
from sklearn.linear_model import LinearRegression
import numpy as np

def calcular_vif(df):
    """
    Calcula o VIF (Variance Inflation Factor) manualmente para cada coluna do DataFrame.
    """
    vif_dict = {}
    X = df.values

    for i in range(df.shape[1]):
        y = X[:, i]  # vari√°vel alvo (uma coluna)
        X_outras = np.delete(X, i, axis=1)  # todas as outras vari√°veis

        modelo = LinearRegression()
        modelo.fit(X_outras, y)
        r2 = modelo.score(X_outras, y)

        vif = 1 / (1 - r2)
        vif_dict[df.columns[i]] = vif

    return pd.DataFrame({"Vari√°vel": list(vif_dict.keys()), "VIF": list(vif_dict.values())}).sort_values(by="VIF", ascending=False)

# Executa o c√°lculo de VIF nas vari√°veis num√©ricas
vif_df = calcular_vif(df_numericas)

print("üìä Fatores de Infla√ß√£o de Vari√¢ncia (VIF) - Calculados sem statsmodels:\")
print(vif_df)
```

## 6. Codifica√ß√£o e Exporta√ß√£o Final

Esta se√ß√£o finaliza o pr√©-processamento dos dados, abordando o mapeamento da vari√°vel target e a codifica√ß√£o das vari√°veis categ√≥ricas, culminando na exporta√ß√£o do dataset final pronto para a fase de modelagem.

### 6.1. Mapeamento do Target (`score_credito`)

Antes de aplicar o One-Hot Encoding nas vari√°veis categ√≥ricas, a vari√°vel target `score_credito` foi mapeada para valores num√©ricos ordinais: `Poor = 0`, `Standard = 1` e `Good = 2`. Essa etapa √© crucial para preparar o target para algoritmos de Machine Learning, que geralmente exigem entradas num√©ricas. Ao mapear o target separadamente, evitamos que ele seja erroneamente codificado junto com as features categ√≥ricas, mantendo uma distin√ß√£o clara entre features e target. O dataset final j√° ter√° o target no formato correto, facilitando a divis√£o em conjuntos de treino e teste na etapa de modelagem.

```python
# Faz uma c√≥pia do df_categoricas para n√£o alterar o original
df_categoricas_mapeado = df_categoricas.copy()

# Mapeia a coluna score_credito diretamente
mapa_target = {"Poor": 0, "Standard": 1, "Good": 2}
df_categoricas_mapeado["score_credito"] = df_categoricas_mapeado["score_credito"].map(mapa_target)

# Confere o resultado
df_categoricas_mapeado[["score_credito"]].head()
```

### 6.2. One-Hot Encoding das Vari√°veis Categ√≥ricas

As vari√°veis categ√≥ricas restantes (exceto o target j√° mapeado) foram codificadas utilizando a t√©cnica de One-Hot Encoding. Esta t√©cnica transforma cada categoria em uma nova coluna bin√°ria (0 ou 1), o que √© essencial para que algoritmos de Machine Learning possam processar vari√°veis categ√≥ricas. A op√ß√£o `drop_first=True` foi utilizada para evitar a armadilha da multicolinearidade, removendo uma das categorias de cada vari√°vel original. Isso garante que o modelo n√£o seja prejudicado por vari√°veis linearmente dependentes.

```python
# Faz uma c√≥pia do df_categoricas_mapeado
df_categoricas_get_dummies = df_categoricas_mapeado.copy()

# Aplica get_dummies em todas as colunas categ√≥ricas, exceto o target
df_categoricas_get_dummies = pd.get_dummies(
    df_categoricas_get_dummies.drop(columns=["score_credito"]), 
    drop_first=True
)

# Confere as primeiras linhas
df_categoricas_get_dummies.head()
```

### 6.3. Concatena√ß√£o e Exporta√ß√£o do Dataset Final

Finalmente, as vari√°veis num√©ricas (j√° tratadas e transformadas) e as vari√°veis categ√≥ricas (j√° codificadas) foram concatenadas para formar o dataset final. A vari√°vel target (`score_credito`), j√° mapeada numericamente, foi adicionada como a √∫ltima coluna. O DataFrame resultante, `df_final`, est√° agora completamente pr√©-processado e pronto para ser utilizado na fase de modelagem. Este dataset foi exportado para um arquivo CSV, garantindo que todas as transforma√ß√µes e tratamentos sejam persistidos e possam ser facilmente carregados para o treinamento de modelos.

```python
# Concatena num√©ricas e categ√≥ricas codificadas
df_final = pd.concat([df_numericas, df_categoricas_get_dummies], axis=1)

# Adiciona o target (j√° mapeado) como √∫ltima coluna
df_final["score_credito"] = df_categoricas_mapeado["score_credito"]

# Conferir o DataFrame final
df_final.info()

# Exibir as primeiras linhas do DataFrame final
df_final.head()

# Exportar o DataFrame final para um arquivo CSV
df_final.to_csv("df_processado_final.csv", index=False)
```

## 7. Conclus√£o

Este documento detalhou exaustivamente as etapas de pr√©-processamento de dados, tratamento de valores ausentes e inconsist√™ncias, feature engineering e an√°lise de multicolinearidade realizadas no projeto de previs√£o de score de cr√©dito. Cada decis√£o foi justificada com base em an√°lises estat√≠sticas e melhores pr√°ticas de Machine Learning, visando a constru√ß√£o de um modelo robusto e confi√°vel. O dataset final, `df_processado_final.csv`, est√° agora preparado para a fase de modelagem, onde diferentes algoritmos poder√£o ser aplicados e avaliados para prever o score de cr√©dito com alta precis√£o e interpretabilidade. A documenta√ß√£o serve como um guia completo para futuras itera√ß√µes e para garantir a reprodutibilidade do pipeline de dados.

## 5. An√°lise de Correla√ß√£o e Multicolinearidade

Esta se√ß√£o descreve a an√°lise de correla√ß√£o entre as vari√°veis num√©ricas e o target, bem como a investiga√ß√£o de multicolinearidade entre as features, utilizando o VIF (Variance Inflation Factor).

### 5.1. Correla√ß√£o com o Target (Pearson e Spearman)

Foram calculadas as correla√ß√µes de Pearson e Spearman entre as vari√°veis num√©ricas e o target (`score_credito_num`). A correla√ß√£o de Pearson mede a rela√ß√£o linear, enquanto a de Spearman mede a rela√ß√£o monot√¥nica, sendo mais robusta a outliers e distribui√ß√µes n√£o normais. A an√°lise revelou que nenhuma vari√°vel apresentava uma correla√ß√£o extremamente alta com o target, o que √© um bom indicativo para evitar *data leakage* e garantir que o modelo precise de m√∫ltiplas features para fazer suas previs√µes. Os resultados foram ordenados pelo valor absoluto da correla√ß√£o para identificar as vari√°veis mais influentes.

```python
# Calcula a correla√ß√£o de Pearson
corr_pearson = df_numericas_e_maptarget.corr(method="pearson")

# Ordena as correla√ß√µes do target pelo valor absoluto, mas preserva o sinal
corr_target_pearson = corr_pearson["score_credito_num"]
corr_target_pearson = corr_target_pearson.reindex(corr_target_pearson.abs().sort_values(ascending=False).index)

print("Correla√ß√£o do target (score_credito_num) com as vari√°veis (Pearson - ordenada pelo valor absoluto):\n")
print(corr_target_pearson)

# Calcula a correla√ß√£o de Spearman
corr_spearman = df_numericas_e_maptarget.corr(method="spearman")

# Ordena as correla√ß√µes do target pelo valor absoluto, mas preserva o sinal
corr_target_spearman = corr_spearman["score_credito_num"]
corr_target_spearman = corr_target_spearman.reindex(corr_target_spearman.abs().sort_values(ascending=False).index)

print("\nCorrela√ß√£o do target (score_credito_num) com as vari√°veis (Spearman - ordenada pelo valor absoluto):\n")
print(corr_target_spearman)
```

### 5.2. An√°lise de VIF (Variance Inflation Factor)

Para investigar a multicolinearidade entre as vari√°veis num√©ricas, foi calculado o VIF (Variance Inflation Factor). O VIF mede o quanto a vari√¢ncia de um coeficiente de regress√£o estimado √© inflacionada devido √† multicolinearidade. Valores de VIF acima de 5 ou 10 geralmente indicam problemas. A an√°lise mostrou que todas as vari√°veis apresentavam valores de VIF entre 1.00 e 2.50, o que √© considerado um n√≠vel saud√°vel de correla√ß√£o e muito abaixo dos limites problem√°ticos. Mesmo a alta correla√ß√£o observada entre `renda_anual` e `salario_liquido_mensal` n√£o resultou em um VIF elevado, indicando que ambas as vari√°veis poderiam ser mantidas sem causar problemas significativos de multicolinearidade no modelo. Diante disso, decidiu-se n√£o remover nenhuma vari√°vel neste momento, deixando eventuais ajustes de sele√ß√£o de vari√°veis para serem avaliados ap√≥s os primeiros testes de modelagem.

```python
from sklearn.linear_model import LinearRegression
import numpy as np

def calcular_vif(df):
    """
    Calcula o VIF (Variance Inflation Factor) manualmente para cada coluna do DataFrame.
    """
    vif_dict = {}
    X = df.values

    for i in range(df.shape[1]):
        y = X[:, i]  # vari√°vel alvo (uma coluna)
        X_outras = np.delete(X, i, axis=1)  # todas as outras vari√°veis

        modelo = LinearRegression()
        modelo.fit(X_outras, y)
        r2 = modelo.score(X_outras, y)

        vif = 1 / (1 - r2)
        vif_dict[df.columns[i]] = vif

    return pd.DataFrame({"Vari√°vel": list(vif_dict.keys()), "VIF": list(vif_dict.values())}).sort_values(by="VIF", ascending=False)

# Executa o c√°lculo de VIF nas vari√°veis num√©ricas
vif_df = calcular_vif(df_numericas)

print("üìä Fatores de Infla√ß√£o de Vari√¢ncia (VIF) - Calculados sem statsmodels:\")
print(vif_df)
```

## 6. Codifica√ß√£o e Exporta√ß√£o Final

Esta se√ß√£o finaliza o pr√©-processamento dos dados, abordando o mapeamento da vari√°vel target e a codifica√ß√£o das vari√°veis categ√≥ricas, culminando na exporta√ß√£o do dataset final pronto para a fase de modelagem.

### 6.1. Mapeamento do Target (`score_credito`)

Antes de aplicar o One-Hot Encoding nas vari√°veis categ√≥ricas, a vari√°vel target `score_credito` foi mapeada para valores num√©ricos ordinais: `Poor = 0`, `Standard = 1` e `Good = 2`. Essa etapa √© crucial para preparar o target para algoritmos de Machine Learning, que geralmente exigem entradas num√©ricas. Ao mapear o target separadamente, evitamos que ele seja erroneamente codificado junto com as features categ√≥ricas, mantendo uma distin√ß√£o clara entre features e target. O dataset final j√° ter√° o target no formato correto, facilitando a divis√£o em conjuntos de treino e teste na etapa de modelagem.

```python
# Faz uma c√≥pia do df_categoricas para n√£o alterar o original
df_categoricas_mapeado = df_categoricas.copy()

# Mapeia a coluna score_credito diretamente
mapa_target = {"Poor": 0, "Standard": 1, "Good": 2}
df_categoricas_mapeado["score_credito"] = df_categoricas_mapeado["score_credito"].map(mapa_target)

# Confere o resultado
df_categoricas_mapeado[["score_credito"]].head()
```

### 6.2. One-Hot Encoding das Vari√°veis Categ√≥ricas

As vari√°veis categ√≥ricas restantes (exceto o target j√° mapeado) foram codificadas utilizando a t√©cnica de One-Hot Encoding. Esta t√©cnica transforma cada categoria em uma nova coluna bin√°ria (0 ou 1), o que √© essencial para que algoritmos de Machine Learning possam processar vari√°veis categ√≥ricas. A op√ß√£o `drop_first=True` foi utilizada para evitar a armadilha da multicolinearidade, removendo uma das categorias de cada vari√°vel original. Isso garante que o modelo n√£o seja prejudicado por vari√°veis linearmente dependentes.

```python
# Faz uma c√≥pia do df_categoricas_mapeado
df_categoricas_get_dummies = df_categoricas_mapeado.copy()

# Aplica get_dummies em todas as colunas categ√≥ricas, exceto o target
df_categoricas_get_dummies = pd.get_dummies(
    df_categoricas_get_dummies.drop(columns=["score_credito"]), 
    drop_first=True
)

# Confere as primeiras linhas
df_categoricas_get_dummies.head()
```

### 6.3. Concatena√ß√£o e Exporta√ß√£o do Dataset Final

Finalmente, as vari√°veis num√©ricas (j√° tratadas e transformadas) e as vari√°veis categ√≥ricas (j√° codificadas) foram concatenadas para formar o dataset final. A vari√°vel target (`score_credito`), j√° mapeada numericamente, foi adicionada como a √∫ltima coluna. O DataFrame resultante, `df_final`, est√° agora completamente pr√©-processado e pronto para ser utilizado na fase de modelagem. Este dataset foi exportado para um arquivo CSV, garantindo que todas as transforma√ß√µes e tratamentos sejam persistidos e possam ser facilmente carregados para o treinamento de modelos.

```python
# Concatena num√©ricas e categ√≥ricas codificadas
df_final = pd.concat([df_numericas, df_categoricas_get_dummies], axis=1)

# Adiciona o target (j√° mapeado) como √∫ltima coluna
df_final["score_credito"] = df_categoricas_mapeado["score_credito"]

# Conferir o DataFrame final
df_final.info()

# Exibir as primeiras linhas do DataFrame final
df_final.head()

# Exportar o DataFrame final para um arquivo CSV
df_final.to_csv("df_processado_final.csv", index=False)
```

## 7. Conclus√£o

Este documento detalhou exaustivamente as etapas de pr√©-processamento de dados, tratamento de valores ausentes e inconsist√™ncias, feature engineering e an√°lise de multicolinearidade realizadas no projeto de previs√£o de score de cr√©dito. Cada decis√£o foi justificada com base em an√°lises estat√≠sticas e melhores pr√°ticas de Machine Learning, visando a constru√ß√£o de um modelo robusto e confi√°vel. O dataset final, `df_processado_final.csv`, est√° agora preparado para a fase de modelagem, onde diferentes algoritmos poder√£o ser aplicados e avaliados para prever o score de cr√©dito com alta precis√£o e interpretabilidade. A documenta√ß√£o serve como um guia completo para futuras itera√ß√µes e para garantir a reprodutibilidade do pipeline de dados.

## 6. Codifica√ß√£o e Exporta√ß√£o Final

Esta se√ß√£o finaliza o pr√©-processamento dos dados, abordando o mapeamento da vari√°vel target e a codifica√ß√£o das vari√°veis categ√≥ricas, culminando na exporta√ß√£o do dataset final pronto para a fase de modelagem.

### 6.1. Mapeamento do Target (`score_credito`)

Antes de aplicar o One-Hot Encoding nas vari√°veis categ√≥ricas, a vari√°vel target `score_credito` foi mapeada para valores num√©ricos ordinais: `Poor = 0`, `Standard = 1` e `Good = 2`. Essa etapa √© crucial para preparar o target para algoritmos de Machine Learning, que geralmente exigem entradas num√©ricas. Ao mapear o target separadamente, evitamos que ele seja erroneamente codificado junto com as features categ√≥ricas, mantendo uma distin√ß√£o clara entre features e target. O dataset final j√° ter√° o target no formato correto, facilitando a divis√£o em conjuntos de treino e teste na etapa de modelagem.

```python
# Faz uma c√≥pia do df_categoricas para n√£o alterar o original
df_categoricas_mapeado = df_categoricas.copy()

# Mapeia a coluna score_credito diretamente
mapa_target = {"Poor": 0, "Standard": 1, "Good": 2}
df_categoricas_mapeado["score_credito"] = df_categoricas_mapeado["score_credito"].map(mapa_target)

# Confere o resultado
df_categoricas_mapeado[["score_credito"]].head()
```

### 6.2. One-Hot Encoding das Vari√°veis Categ√≥ricas

As vari√°veis categ√≥ricas restantes (exceto o target j√° mapeado) foram codificadas utilizando a t√©cnica de One-Hot Encoding. Esta t√©cnica transforma cada categoria em uma nova coluna bin√°ria (0 ou 1), o que √© essencial para que algoritmos de Machine Learning possam processar vari√°veis categ√≥ricas. A op√ß√£o `drop_first=True` foi utilizada para evitar a armadilha da multicolinearidade, removendo uma das categorias de cada vari√°vel original. Isso garante que o modelo n√£o seja prejudicado por vari√°veis linearmente dependentes.

```python
# Faz uma c√≥pia do df_categoricas_mapeado
df_categoricas_get_dummies = df_categoricas_mapeado.copy()

# Aplica get_dummies em todas as colunas categ√≥ricas, exceto o target
df_categoricas_get_dummies = pd.get_dummies(
    df_categoricas_get_dummies.drop(columns=["score_credito"]), 
    drop_first=True
)

# Confere as primeiras linhas
df_categoricas_get_dummies.head()
```

### 6.3. Concatena√ß√£o e Exporta√ß√£o do Dataset Final

Finalmente, as vari√°veis num√©ricas (j√° tratadas e transformadas) e as vari√°veis categ√≥ricas (j√° codificadas) foram concatenadas para formar o dataset final. A vari√°vel target (`score_credito`), j√° mapeada numericamente, foi adicionada como a √∫ltima coluna. O DataFrame resultante, `df_final`, est√° agora completamente pr√©-processado e pronto para ser utilizado na fase de modelagem. Este dataset foi exportado para um arquivo CSV, garantindo que todas as transforma√ß√µes e tratamentos sejam persistidos e possam ser facilmente carregados para o treinamento de modelos.

```python
# Concatena num√©ricas e categ√≥ricas codificadas
df_final = pd.concat([df_numericas, df_categoricas_get_dummies], axis=1)

# Adiciona o target (j√° mapeado) como √∫ltima coluna
df_final["score_credito"] = df_categoricas_mapeado["score_credito"]

# Conferir o DataFrame final
df_final.info()

# Exibir as primeiras linhas do DataFrame final
df_final.head()

# Exportar o DataFrame final para um arquivo CSV
df_final.to_csv("df_processado_final.csv", index=False)
```

## 7. Conclus√£o

Este documento detalhou exaustivamente as etapas de pr√©-processamento de dados, tratamento de valores ausentes e inconsist√™ncias, feature engineering e an√°lise de multicolinearidade realizadas no projeto de previs√£o de score de cr√©dito. Cada decis√£o foi justificada com base em an√°lises estat√≠sticas e melhores pr√°ticas de Machine Learning, visando a constru√ß√£o de um modelo robusto e confi√°vel. O dataset final, `df_processado_final.csv`, est√° agora preparado para a fase de modelagem, onde diferentes algoritmos poder√£o ser aplicados e avaliados para prever o score de cr√©dito com alta precis√£o e interpretabilidade. A documenta√ß√£o serve como um guia completo para futuras itera√ß√µes e para garantir a reprodutibilidade do pipeline de dados.

## 4. Feature Engineering e Transforma√ß√µes

Esta se√ß√£o detalha as t√©cnicas de feature engineering e transforma√ß√µes aplicadas √†s vari√°veis num√©ricas do dataset. O objetivo √© otimizar a representa√ß√£o dos dados para melhorar o desempenho dos modelos de Machine Learning, lidando com outliers, assimetrias e a cria√ß√£o de novas features quando aplic√°vel.

### 4.1. Coluna `idade`: Tratamento de Outliers e Discretiza√ß√£o

A coluna `idade` foi analisada para identificar e tratar outliers. A presen√ßa de valores extremos pode distorcer as an√°lises estat√≠sticas e o treinamento de modelos. Ap√≥s a identifica√ß√£o, os outliers foram tratados utilizando a t√©cnica de winsoriza√ß√£o, onde valores abaixo do percentil 1 e acima do percentil 99 foram substitu√≠dos pelos valores desses percentis, respectivamente. Isso ajuda a mitigar o impacto de valores extremos sem remover os dados completamente.

Al√©m do tratamento de outliers, a coluna `idade` foi discretizada em faixas et√°rias. A discretiza√ß√£o transforma uma vari√°vel cont√≠nua em uma vari√°vel categ√≥rica ordinal, o que pode ser ben√©fico para alguns modelos que n√£o lidam bem com a linearidade ou para capturar rela√ß√µes n√£o lineares. As faixas et√°rias foram definidas de forma a criar grupos significativos para a an√°lise de cr√©dito.

```python
# Exibir estat√≠sticas descritivas da coluna idade
print("=== Estat√≠sticas descritivas da coluna idade ===")
print(df_numericas["idade"].describe())

# Identificar e tratar outliers (exemplo com winsoriza√ß√£o)
Q1 = df_numericas["idade"].quantile(0.01)
Q3 = df_numericas["idade"].quantile(0.99)
df_numericas["idade"] = df_numericas["idade"].clip(lower=Q1, upper=Q3)

# Discretiza√ß√£o da coluna idade em faixas et√°rias
bins = [18, 25, 35, 45, 55, 65, 100]
labels = ["18-24", "25-34", "35-44", "45-54", "55-64", "65+"]
df_numericas["faixa_etaria"] = pd.cut(df_numericas["idade"], bins=bins, labels=labels, right=False)

# Conferir a distribui√ß√£o das novas faixas et√°rias
print("\n=== Distribui√ß√£o de faixas et√°rias ===")
print(df_numericas["faixa_etaria"].value_counts(normalize=True) * 100)
```

### 4.2. Coluna `renda_anual`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `renda_anual` apresentava uma distribui√ß√£o altamente assim√©trica e a presen√ßa de outliers extremos, o que √© comum em vari√°veis de renda. Para mitigar o impacto desses outliers e normalizar a distribui√ß√£o, foi aplicada uma transforma√ß√£o logar√≠tmica (log1p, que lida bem com valores zero ou pr√≥ximos de zero). Antes da transforma√ß√£o, os outliers foram tratados com winsoriza√ß√£o, substituindo valores abaixo do percentil 1 e acima do percentil 99 pelos respectivos limites. Essa abordagem ajuda a reduzir a influ√™ncia de valores extremos e a tornar a distribui√ß√£o mais pr√≥xima de uma normal, o que √© ben√©fico para muitos algoritmos de Machine Learning.

```python
# Exibir estat√≠sticas descritivas da coluna renda_anual
print("=== Estat√≠sticas descritivas da coluna renda_anual ===")
print(df_numericas["renda_anual"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["renda_anual"].quantile(0.01)
Q3 = df_numericas["renda_anual"].quantile(0.99)
df_numericas["renda_anual"] = df_numericas["renda_anual"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["renda_anual_log"] = np.log1p(df_numericas["renda_anual"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna renda_anual_log ===")
print(df_numericas["renda_anual_log"].describe())
```

### 4.3. Coluna `salario_liquido_mensal`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

Similar √† `renda_anual`, a coluna `salario_liquido_mensal` tamb√©m exibia assimetria e outliers. O tratamento seguiu a mesma l√≥gica: winsoriza√ß√£o nos percentis 1 e 99 para limitar os valores extremos, seguida pela aplica√ß√£o da transforma√ß√£o logar√≠tmica (log1p). Essa transforma√ß√£o √© eficaz para reduzir a assimetria e estabilizar a vari√¢ncia, tornando a vari√°vel mais adequada para modelos que assumem distribui√ß√µes mais sim√©tricas.

```python
# Exibir estat√≠sticas descritivas da coluna salario_liquido_mensal
print("=== Estat√≠sticas descritivas da coluna salario_liquido_mensal ===")
print(df_numericas["salario_liquido_mensal"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["salario_liquido_mensal"].quantile(0.01)
Q3 = df_numericas["salario_liquido_mensal"].quantile(0.99)
df_numericas["salario_liquido_mensal"] = df_numericas["salario_liquido_mensal"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["salario_liquido_mensal_log"] = np.log1p(df_numericas["salario_liquido_mensal"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna salario_liquido_mensal_log ===")
print(df_numericas["salario_liquido_mensal_log"].describe())
```

### 4.4. Coluna `qtd_contas_bancarias`: Tratamento de Outliers

A coluna `qtd_contas_bancarias` foi analisada para a presen√ßa de outliers. Embora seja uma vari√°vel discreta, valores excessivamente altos podem indicar anomalias ou clientes com perfis muito espec√≠ficos que podem distorcer o modelo. O tratamento de outliers foi realizado por winsoriza√ß√£o nos percentis 1 e 99, garantindo que os valores extremos fossem limitados sem perder a informa√ß√£o da distribui√ß√£o central.

```python
# Exibir estat√≠sticas descritivas da coluna qtd_contas_bancarias
print("=== Estat√≠sticas descritivas da coluna qtd_contas_bancarias ===")
print(df_numericas["qtd_contas_bancarias"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["qtd_contas_bancarias"].quantile(0.01)
Q3 = df_numericas["qtd_contas_bancarias"].quantile(0.99)
df_numericas["qtd_contas_bancarias"] = df_numericas["qtd_contas_bancarias"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna qtd_contas_bancarias ap√≥s tratamento ===")
print(df_numericas["qtd_contas_bancarias"].describe())
```

### 4.5. Coluna `qtd_cartoes_credito`: Tratamento de Outliers

Similar √† `qtd_contas_bancarias`, a coluna `qtd_cartoes_credito` tamb√©m √© uma vari√°vel discreta que pode conter outliers. O tratamento foi realizado com winsoriza√ß√£o nos percentis 1 e 99 para limitar a influ√™ncia de valores extremos, como um n√∫mero excepcionalmente alto ou baixo de cart√µes de cr√©dito, que poderiam ser ru√≠do ou representar casos muito espec√≠ficos que n√£o generalizam bem.

```python
# Exibir estat√≠sticas descritivas da coluna qtd_cartoes_credito
print("=== Estat√≠sticas descritivas da coluna qtd_cartoes_credito ===")
print(df_numericas["qtd_cartoes_credito"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["qtd_cartoes_credito"].quantile(0.01)
Q3 = df_numericas["qtd_cartoes_credito"].quantile(0.99)
df_numericas["qtd_cartoes_credito"] = df_numericas["qtd_cartoes_credito"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna qtd_cartoes_credito ap√≥s tratamento ===")
print(df_numericas["qtd_cartoes_credito"].describe())
```

### 4.6. Coluna `taxa_juros`: Tratamento de Outliers

A coluna `taxa_juros` √© uma vari√°vel num√©rica cont√≠nua que pode apresentar outliers, especialmente taxas de juros muito altas ou muito baixas. O tratamento foi feito por winsoriza√ß√£o nos percentis 1 e 99, a fim de suavizar a influ√™ncia desses valores extremos e garantir que a distribui√ß√£o da vari√°vel seja mais representativa para o treinamento do modelo.

```python
# Exibir estat√≠sticas descritivas da coluna taxa_juros
print("=== Estat√≠sticas descritivas da coluna taxa_juros ===")
print(df_numericas["taxa_juros"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["taxa_juros"].quantile(0.01)
Q3 = df_numericas["taxa_juros"].quantile(0.99)
df_numericas["taxa_juros"] = df_numericas["taxa_juros"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna taxa_juros ap√≥s tratamento ===")
print(df_numericas["taxa_juros"].describe())
```

### 4.7. Coluna `qtd_emprestimos`: Tratamento de Outliers

A coluna `qtd_emprestimos` representa o n√∫mero de empr√©stimos e, como outras vari√°veis de contagem, pode ter outliers. A winsoriza√ß√£o nos percentis 1 e 99 foi aplicada para limitar a influ√™ncia de clientes com um n√∫mero excepcionalmente alto ou baixo de empr√©stimos, garantindo que o modelo n√£o seja excessivamente influenciado por esses casos extremos.

```python
# Exibir estat√≠sticas descritivas da coluna qtd_emprestimos
print("=== Estat√≠sticas descritivas da coluna qtd_emprestimos ===")
print(df_numericas["qtd_emprestimos"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["qtd_emprestimos"].quantile(0.01)
Q3 = df_numericas["qtd_emprestimos"].quantile(0.99)
df_numericas["qtd_emprestimos"] = df_numericas["qtd_emprestimos"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna qtd_emprestimos ap√≥s tratamento ===")
print(df_numericas["qtd_emprestimos"].describe())
```

### 4.8. Coluna `dias_atraso_pagamento`: Tratamento de Outliers

A coluna `dias_atraso_pagamento` indica o n√∫mero de dias de atraso em pagamentos. Valores muito altos nesta coluna s√£o outliers cr√≠ticos, pois representam um comportamento de cr√©dito de alto risco. A winsoriza√ß√£o nos percentis 1 e 99 foi utilizada para limitar esses valores extremos, mantendo a informa√ß√£o de atraso, mas suavizando o impacto dos atrasos mais severos, que poderiam dominar o modelo.

```python
# Exibir estat√≠sticas descritivas da coluna dias_atraso_pagamento
print("=== Estat√≠sticas descritivas da coluna dias_atraso_pagamento ===")
print(df_numericas["dias_atraso_pagamento"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["dias_atraso_pagamento"].quantile(0.01)
Q3 = df_numericas["dias_atraso_pagamento"].quantile(0.99)
df_numericas["dias_atraso_pagamento"] = df_numericas["dias_atraso_pagamento"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna dias_atraso_pagamento ap√≥s tratamento ===")
print(df_numericas["dias_atraso_pagamento"].describe())
```

### 4.9. Coluna `qtd_pagamentos_atrasados`: Tratamento de Outliers

Similar √† coluna de dias de atraso, a `qtd_pagamentos_atrasados` tamb√©m √© uma m√©trica de risco. Outliers nesta coluna (um n√∫mero muito alto de pagamentos atrasados) foram tratados com winsoriza√ß√£o nos percentis 1 e 99. Essa t√©cnica ajuda a manter a representatividade da vari√°vel sem permitir que poucos casos extremos distor√ßam o aprendizado do modelo.

```python
# Exibir estat√≠sticas descritivas da coluna qtd_pagamentos_atrasados
print("=== Estat√≠sticas descritivas da coluna qtd_pagamentos_atrasados ===")
print(df_numericas["qtd_pagamentos_atrasados"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["qtd_pagamentos_atrasados"].quantile(0.01)
Q3 = df_numericas["qtd_pagamentos_atrasados"].quantile(0.99)
df_numericas["qtd_pagamentos_atrasados"] = df_numericas["qtd_pagamentos_atrasados"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna qtd_pagamentos_atrasados ap√≥s tratamento ===")
print(df_numericas["qtd_pagamentos_atrasados"].describe())
```

### 4.10. Coluna `variacao_limite_credito`: Tratamento de Outliers

A coluna `variacao_limite_credito` pode apresentar outliers em ambas as extremidades (varia√ß√µes muito positivas ou muito negativas). A winsoriza√ß√£o nos percentis 1 e 99 foi aplicada para limitar esses valores extremos, garantindo que a vari√°vel contribua de forma mais est√°vel para o modelo, sem ser excessivamente influenciada por mudan√ßas at√≠picas no limite de cr√©dito.

```python
# Exibir estat√≠sticas descritivas da coluna variacao_limite_credito
print("=== Estat√≠sticas descritivas da coluna variacao_limite_credito ===")
print(df_numericas["variacao_limite_credito"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["variacao_limite_credito"].quantile(0.01)
Q3 = df_numericas["variacao_limite_credito"].quantile(0.99)
df_numericas["variacao_limite_credito"] = df_numericas["variacao_limite_credito"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna variacao_limite_credito ap√≥s tratamento ===")
print(df_numericas["variacao_limite_credito"].describe())
```

### 4.11. Coluna `qtd_consultas_credito`: Tratamento de Outliers

A coluna `qtd_consultas_credito` representa o n√∫mero de consultas de cr√©dito. Um n√∫mero excessivamente alto de consultas pode indicar um comportamento de busca de cr√©dito arriscado. Outliers nesta coluna foram tratados com winsoriza√ß√£o nos percentis 1 e 99, para mitigar o impacto de valores extremos e garantir que a vari√°vel seja mais robusta para o modelo.

```python
# Exibir estat√≠sticas descritivas da coluna qtd_consultas_credito
print("=== Estat√≠sticas descritivas da coluna qtd_consultas_credito ===")
print(df_numericas["qtd_consultas_credito"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["qtd_consultas_credito"].quantile(0.01)
Q3 = df_numericas["qtd_consultas_credito"].quantile(0.99)
df_numericas["qtd_consultas_credito"] = df_numericas["qtd_consultas_credito"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna qtd_consultas_credito ap√≥s tratamento ===")
print(df_numericas["qtd_consultas_credito"].describe())
```

### 4.12. Coluna `divida_pendente`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `divida_pendente` √© uma vari√°vel financeira que frequentemente apresenta assimetria e outliers. O tratamento incluiu a winsoriza√ß√£o nos percentis 1 e 99 para limitar os valores extremos, seguida pela aplica√ß√£o da transforma√ß√£o logar√≠tmica (log1p). Essa combina√ß√£o ajuda a normalizar a distribui√ß√£o da d√≠vida pendente, tornando-a mais adequada para modelos que assumem distribui√ß√µes mais sim√©tricas e reduzindo a influ√™ncia de d√≠vidas excepcionalmente altas.

```python
# Exibir estat√≠sticas descritivas da coluna divida_pendente
print("=== Estat√≠sticas descritivas da coluna divida_pendente ===")
print(df_numericas["divida_pendente"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["divida_pendente"].quantile(0.01)
Q3 = df_numericas["divida_pendente"].quantile(0.99)
df_numericas["divida_pendente"] = df_numericas["divida_pendente"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["divida_pendente_log"] = np.log1p(df_numericas["divida_pendente"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna divida_pendente_log ===")
print(df_numericas["divida_pendente_log"].describe())
```

### 4.13. Coluna `percentual_utilizacao_credito`: Tratamento de Outliers

A coluna `percentual_utilizacao_credito` indica o qu√£o pr√≥ximo o cliente est√° do seu limite de cr√©dito. Valores muito altos (pr√≥ximos a 100%) ou muito baixos podem ser outliers. A winsoriza√ß√£o nos percentis 1 e 99 foi aplicada para limitar esses valores extremos, garantindo que a vari√°vel seja mais representativa do comportamento geral de utiliza√ß√£o de cr√©dito e menos suscet√≠vel a casos at√≠picos.

```python
# Exibir estat√≠sticas descritivas da coluna percentual_utilizacao_credito
print("=== Estat√≠sticas descritivas da coluna percentual_utilizacao_credito ===")
print(df_numericas["percentual_utilizacao_credito"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["percentual_utilizacao_credito"].quantile(0.01)
Q3 = df_numericas["percentual_utilizacao_credito"].quantile(0.99)
df_numericas["percentual_utilizacao_credito"] = df_numericas["percentual_utilizacao_credito"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna percentual_utilizacao_credito ap√≥s tratamento ===")
print(df_numericas["percentual_utilizacao_credito"].describe())
```

### 4.14. Coluna `total_emprestimos_mensal`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `total_emprestimos_mensal` (total de empr√©stimos mensais) √© outra vari√°vel financeira que tende a ter uma distribui√ß√£o assim√©trica e outliers. O tratamento envolveu a winsoriza√ß√£o nos percentis 1 e 99, seguida pela transforma√ß√£o logar√≠tmica (log1p). Essa abordagem padroniza a vari√°vel, reduzindo a assimetria e a influ√™ncia de valores extremos, o que √© fundamental para a estabilidade do modelo.

```python
# Exibir estat√≠sticas descritivas da coluna total_emprestimos_mensal
print("=== Estat√≠sticas descritivas da coluna total_emprestimos_mensal ===")
print(df_numericas["total_emprestimos_mensal"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["total_emprestimos_mensal"].quantile(0.01)
Q3 = df_numericas["total_emprestimos_mensal"].quantile(0.99)
df_numericas["total_emprestimos_mensal"] = df_numericas["total_emprestimos_mensal"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["total_emprestimos_mensal_log"] = np.log1p(df_numericas["total_emprestimos_mensal"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna total_emprestimos_mensal_log ===")
print(df_numericas["total_emprestimos_mensal_log"].describe())
```

### 4.15. Coluna `valor_investido_mensal`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `valor_investido_mensal` (valor investido mensalmente) tamb√©m √© uma vari√°vel financeira que pode apresentar uma distribui√ß√£o assim√©trica e outliers. O tratamento foi realizado com winsoriza√ß√£o nos percentis 1 e 99, seguida pela transforma√ß√£o logar√≠tmica (log1p). Essa estrat√©gia visa normalizar a distribui√ß√£o e reduzir a influ√™ncia de valores de investimento excepcionalmente altos, que poderiam enviesar o modelo.

```python
# Exibir estat√≠sticas descritivas da coluna valor_investido_mensal
print("=== Estat√≠sticas descritivas da coluna valor_investido_mensal ===")
print(df_numericas["valor_investido_mensal"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["valor_investido_mensal"].quantile(0.01)
Q3 = df_numericas["valor_investido_mensal"].quantile(0.99)
df_numericas["valor_investido_mensal"] = df_numericas["valor_investido_mensal"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["valor_investido_mensal_log"] = np.log1p(df_numericas["valor_investido_mensal"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna valor_investido_mensal_log ===")
print(df_numericas["valor_investido_mensal_log"].describe())
```

### 4.16. Coluna `saldo_mensal`: Tratamento de Outliers e Transforma√ß√£o Logar√≠tmica

A coluna `saldo_mensal` (saldo mensal) √© outra vari√°vel financeira que pode se beneficiar do tratamento de outliers e da transforma√ß√£o logar√≠tmica. A winsoriza√ß√£o nos percentis 1 e 99 foi aplicada para limitar os valores extremos, seguida pela transforma√ß√£o logar√≠tmica (log1p). Essa abordagem ajuda a estabilizar a vari√¢ncia e a normalizar a distribui√ß√£o, tornando a vari√°vel mais adequada para o treinamento do modelo.

```python
# Exibir estat√≠sticas descritivas da coluna saldo_mensal
print("=== Estat√≠sticas descritivas da coluna saldo_mensal ===")
print(df_numericas["saldo_mensal"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["saldo_mensal"].quantile(0.01)
Q3 = df_numericas["saldo_mensal"].quantile(0.99)
df_numericas["saldo_mensal"] = df_numericas["saldo_mensal"].clip(lower=Q1, upper=Q3)

# Aplicar transforma√ß√£o logar√≠tmica (log1p para lidar com valores zero)
df_numericas["saldo_mensal_log"] = np.log1p(df_numericas["saldo_mensal"])

# Exibir estat√≠sticas descritivas da coluna transformada
print("\n=== Estat√≠sticas descritivas da coluna saldo_mensal_log ===")
print(df_numericas["saldo_mensal_log"].describe())
```

### 4.17. Coluna `tempo_historico_credito_meses`: Tratamento de Outliers

A coluna `tempo_historico_credito_meses` (tempo de hist√≥rico de cr√©dito em meses) √© uma vari√°vel importante para a avalia√ß√£o de cr√©dito. Outliers nesta coluna (tempos de hist√≥rico muito curtos ou muito longos) foram tratados com winsoriza√ß√£o nos percentis 1 e 99. Essa t√©cnica ajuda a garantir que a vari√°vel seja mais robusta e menos suscet√≠vel a valores extremos que poderiam distorcer a an√°lise de cr√©dito.

```python
# Exibir estat√≠sticas descritivas da coluna tempo_historico_credito_meses
print("=== Estat√≠sticas descritivas da coluna tempo_historico_credito_meses ===")
print(df_numericas["tempo_historico_credito_meses"].describe())

# Identificar e tratar outliers (winsoriza√ß√£o)
Q1 = df_numericas["tempo_historico_credito_meses"].quantile(0.01)
Q3 = df_numericas["tempo_historico_credito_meses"].quantile(0.99)
df_numericas["tempo_historico_credito_meses"] = df_numericas["tempo_historico_credito_meses"].clip(lower=Q1, upper=Q3)

# Conferir estat√≠sticas ap√≥s tratamento
print("\n=== Estat√≠sticas descritivas da coluna tempo_historico_credito_meses ap√≥s tratamento ===")
print(df_numericas["tempo_historico_credito_meses"].describe())
```

## 7. Conclus√£o

Este documento detalhou exaustivamente as etapas de pr√©-processamento de dados, tratamento de valores ausentes e inconsist√™ncias, feature engineering e an√°lise de multicolinearidade realizadas no projeto de previs√£o de score de cr√©dito. Cada decis√£o foi justificada com base em an√°lises estat√≠sticas e melhores pr√°ticas de Machine Learning, visando a constru√ß√£o de um modelo robusto e confi√°vel. O dataset final, `df_processado_final.csv`, est√° agora preparado para a fase de modelagem, onde diferentes algoritmos poder√£o ser aplicados e avaliados para prever o score de cr√©dito com alta precis√£o e interpretabilidade. A documenta√ß√£o serve como um guia completo para futuras itera√ß√µes e para garantir a reprodutibilidade do pipeline de dados.

